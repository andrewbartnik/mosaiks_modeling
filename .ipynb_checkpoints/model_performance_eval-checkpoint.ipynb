{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe56655",
   "metadata": {},
   "source": [
    "# Modeling Agricultural Variables (Incomplete)\n",
    "\n",
    "This notebook contains the modeling approach used by the 2023 MOSAIKS team as part of the University of California, Santa Barbara's Bren School of the Environment & Management Masters of Environmental Data Science (MEDS) Program's Capstone Project. \n",
    "\n",
    "This notebook immediately follows the [feature preprocessing](https://github.com/mosaiks-capstone/Modeling/blob/main/feature_preprocessing.ipynb) notebook used to aggregate and join featurized satellite imagery (generated [here](https://github.com/mosaiks-capstone/Featurization)) and ground truth data (in our case, Crop Forecast Survey (CFS) Data collected by the Zambian Ministry of Agriculture). Our approach accomodates different sampling methods: bootstrapping and block sampling used in combination with RidgeCV's 5 fold cross validation. These additional sampling methods can be used to evaluate model performances, but ensembling the individual bootstrapped models into a final model to generate predictions is incomplete but can be expanded on in future work.\n",
    "\n",
    "To use our modeling approach to make predictions, please see the ![model_predictions notebook]()\n",
    "\n",
    "\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b4f19f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import multiprocessing as mp\n",
    "\n",
    "import geopandas as gpd\n",
    "import pyarrow\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, r2_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.linalg import LinAlgWarning\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import check_random_state, resample\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0591f-583f-4945-b1d2-0323ae715531",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read in Data\n",
    "\n",
    "We first read in the aggregated features and ground-truth data joined in  `feature_preprocessing.ipynb` .\n",
    "\n",
    "The joined data being read in should take on the following form:\n",
    "\n",
    "| spatial_identifier | year | target_1 | target_2 | feature1| feature2 | feature3\n",
    "| ----| ----| ---- | ---- | -- | -- | -- |\n",
    "| 1   | 2016 | 72 | 13 | 1.23 | 3.25 | 0.123\n",
    "| 2   | 2016  | 50 | 7.5 | 0.78| 1.2 | 2.4\n",
    "\n",
    "\n",
    "\n",
    "In our case, our unique spatial_identifiers are `sea_unq`. This enables us to regress `target_1` and `target_2` on our features, using the following equation:\n",
    "\n",
    "$y_{1}$ = $\\beta_{1}$$x_{1}$ + $\\beta_{2}$$x_{2}$ + $\\beta_{3}$$x_{3}$ + $\\beta_{n}$$x_{n}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f593b6f-5740-41de-b785-3a4555428899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sea_unq</th>\n",
       "      <th>0_1</th>\n",
       "      <th>0_2</th>\n",
       "      <th>0_3</th>\n",
       "      <th>0_4</th>\n",
       "      <th>0_5</th>\n",
       "      <th>0_6</th>\n",
       "      <th>0_7</th>\n",
       "      <th>0_8</th>\n",
       "      <th>...</th>\n",
       "      <th>log_sweetpotatoes</th>\n",
       "      <th>log_groundnuts</th>\n",
       "      <th>log_soybeans</th>\n",
       "      <th>loss_ind</th>\n",
       "      <th>drought_loss_ind</th>\n",
       "      <th>flood_loss_ind</th>\n",
       "      <th>animal_loss_ind</th>\n",
       "      <th>pest_loss_ind</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.157999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>5.935403</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.659357</td>\n",
       "      <td>27.807993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>6.299240e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>5.935403</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.493902</td>\n",
       "      <td>27.959205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.008277e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689155</td>\n",
       "      <td>5.935403</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.772690</td>\n",
       "      <td>28.634660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.590917e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>-1.408767</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.905428</td>\n",
       "      <td>27.406446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.113844e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.525729</td>\n",
       "      <td>3.354421</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.962298</td>\n",
       "      <td>27.381719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>388</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>8.372727e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>9.367183</td>\n",
       "      <td>8.098897</td>\n",
       "      <td>7.336848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.652084</td>\n",
       "      <td>25.116478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>389</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>7.513932e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>8.048788</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.966394</td>\n",
       "      <td>22.794290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>390</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>9.321970e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.863267</td>\n",
       "      <td>8.154788</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.240607</td>\n",
       "      <td>23.101535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>391</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>1.540414e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>8.065208</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.485957</td>\n",
       "      <td>24.338360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>392</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>2.518747e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>7.960925</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.286023</td>\n",
       "      <td>23.242993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1369 rows × 12043 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  sea_unq       0_1       0_2       0_3       0_4       0_5  \\\n",
       "0     2016.0        1  0.000000  0.000863  0.000783  0.000000  0.000000   \n",
       "1     2016.0        2  0.000069  0.000863  0.000783  0.000000  0.000002   \n",
       "2     2016.0        7  0.001141  0.000863  0.000783  0.000329  0.000000   \n",
       "3     2016.0        9  0.001131  0.000863  0.000783  0.000006  0.000004   \n",
       "4     2016.0       10  0.001131  0.000863  0.000783  0.000000  0.000000   \n",
       "...      ...      ...       ...       ...       ...       ...       ...   \n",
       "1364  2021.0      388  0.001131  0.000076  0.000106  0.000075  0.000046   \n",
       "1365  2021.0      389  0.001131  0.000863  0.000018  0.000104  0.000202   \n",
       "1366  2021.0      390  0.000821  0.000345  0.000180  0.000227  0.000306   \n",
       "1367  2021.0      391  0.001131  0.000863  0.000353  0.000323  0.000244   \n",
       "1368  2021.0      392  0.001661  0.001136  0.000874  0.000720  0.000429   \n",
       "\n",
       "           0_6       0_7           0_8  ...  log_sweetpotatoes  \\\n",
       "0     0.000000  0.000000  6.157999e-06  ...           6.364023   \n",
       "1     0.000014  0.000047  6.299240e-05  ...           6.364023   \n",
       "2     0.000000  0.000000  1.008277e-03  ...           0.689155   \n",
       "3     0.000010  0.000014  2.590917e-05  ...           6.364023   \n",
       "4     0.000000  0.000000  3.113844e-07  ...           2.525729   \n",
       "...        ...       ...           ...  ...                ...   \n",
       "1364  0.000112  0.000144  8.372727e-04  ...           9.367183   \n",
       "1365  0.000424  0.000395  7.513932e-04  ...           6.364023   \n",
       "1366  0.000457  0.000474  9.321970e-04  ...           7.863267   \n",
       "1367  0.000222  0.000311  1.540414e-03  ...           6.364023   \n",
       "1368  0.000714  0.001367  2.518747e-03  ...           6.364023   \n",
       "\n",
       "      log_groundnuts  log_soybeans  loss_ind  drought_loss_ind  \\\n",
       "0           5.935403      6.565149       0.0               0.0   \n",
       "1           5.935403      6.565149       0.0               0.0   \n",
       "2           5.935403      6.565149       1.0               1.0   \n",
       "3          -1.408767      6.565149       1.0               0.0   \n",
       "4           3.354421      6.565149       1.0               0.0   \n",
       "...              ...           ...       ...               ...   \n",
       "1364        8.098897      7.336848       0.0               0.0   \n",
       "1365        8.048788      6.565149       1.0               1.0   \n",
       "1366        8.154788      6.565149       1.0               0.0   \n",
       "1367        8.065208      6.565149       1.0               0.0   \n",
       "1368        7.960925      6.565149       1.0               0.0   \n",
       "\n",
       "      flood_loss_ind  animal_loss_ind  pest_loss_ind        lat        lon  \n",
       "0                0.0              0.0            0.0 -13.659357  27.807993  \n",
       "1                0.0              0.0            0.0 -13.493902  27.959205  \n",
       "2                0.0              0.0            0.0 -13.772690  28.634660  \n",
       "3                0.0              0.0            0.0 -12.905428  27.406446  \n",
       "4                0.0              0.0            0.0 -12.962298  27.381719  \n",
       "...              ...              ...            ...        ...        ...  \n",
       "1364             0.0              0.0            0.0 -14.652084  25.116478  \n",
       "1365             0.0              0.0            0.0 -13.966394  22.794290  \n",
       "1366             0.0              0.0            0.0 -14.240607  23.101535  \n",
       "1367             1.0              0.0            0.0 -16.485957  24.338360  \n",
       "1368             0.0              0.0            0.0 -16.286023  23.242993  \n",
       "\n",
       "[1369 rows x 12043 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Insert path to joined ground data + features\n",
    "path = \"/capstone/mosaiks/repos/modeling/data/model_directory/SEA_averaged_features_simple_impute_mean.csv\" ## Your path here\n",
    "\n",
    "grouped_features = pd.read_csv(path)\n",
    "grouped_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab389f6-08ba-4f51-9163-28b24c6618e4",
   "metadata": {},
   "source": [
    "### Select Features and Outcomes\n",
    "\n",
    "We then select all observations for each of the columns containing the features. We do the same with our outcome/target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbe3e8ca-f209-4c76-a209-1c49d83c81a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_1</th>\n",
       "      <th>0_2</th>\n",
       "      <th>0_3</th>\n",
       "      <th>0_4</th>\n",
       "      <th>0_5</th>\n",
       "      <th>0_6</th>\n",
       "      <th>0_7</th>\n",
       "      <th>0_8</th>\n",
       "      <th>0_9</th>\n",
       "      <th>0_10</th>\n",
       "      <th>...</th>\n",
       "      <th>999_3</th>\n",
       "      <th>999_4</th>\n",
       "      <th>999_5</th>\n",
       "      <th>999_6</th>\n",
       "      <th>999_7</th>\n",
       "      <th>999_8</th>\n",
       "      <th>999_9</th>\n",
       "      <th>999_10</th>\n",
       "      <th>999_11</th>\n",
       "      <th>999_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.157999e-06</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.274676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115388</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>6.299240e-05</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>0.939709</td>\n",
       "      <td>0.049106</td>\n",
       "      <td>0.039969</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.071531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.008277e-03</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.071531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.590917e-05</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>0.006391</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.071531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.113844e-07</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.071531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0_1       0_2       0_3       0_4       0_5       0_6       0_7  \\\n",
       "0  0.000000  0.000863  0.000783  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000069  0.000863  0.000783  0.000000  0.000002  0.000014  0.000047   \n",
       "2  0.001141  0.000863  0.000783  0.000329  0.000000  0.000000  0.000000   \n",
       "3  0.001131  0.000863  0.000783  0.000006  0.000004  0.000010  0.000014   \n",
       "4  0.001131  0.000863  0.000783  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            0_8       0_9      0_10  ...     999_3     999_4     999_5  \\\n",
       "0  6.157999e-06  0.000207  0.001568  ...  0.060421  1.000000  0.274676   \n",
       "1  6.299240e-05  0.000168  0.001568  ...  0.060421  0.939709  0.049106   \n",
       "2  1.008277e-03  0.001360  0.002211  ...  0.060421  0.006789  1.000000   \n",
       "3  2.590917e-05  0.000110  0.001568  ...  0.060421  0.005561  0.006391   \n",
       "4  3.113844e-07  0.000012  0.001568  ...  0.060421  0.005570  0.006739   \n",
       "\n",
       "      999_6     999_7     999_8     999_9    999_10    999_11    999_12  \n",
       "0  1.000000  0.115388  0.002708  0.001319  0.002867  0.003866  1.000000  \n",
       "1  0.039969  0.004752  0.002671  0.002439  0.002867  0.003866  0.071531  \n",
       "2  1.000000  1.000000  0.000517  0.000343  0.000396  0.003866  0.071531  \n",
       "3  0.004212  0.003235  0.001937  0.001683  0.002867  0.003866  0.071531  \n",
       "4  0.003991  0.002857  0.001979  0.001435  0.002867  0.003866  0.071531  \n",
       "\n",
       "[5 rows x 12000 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features and outcomes:\n",
    "features = grouped_features.iloc[:,2:12002] # adjust to select all feature columns\n",
    "outcomes = grouped_features.iloc[:,12003:12041] #adjust to select all target/outcome variables\n",
    "\n",
    "## Ensure each of the binary target variables are of categorical data types\n",
    "outcomes[\"loss_ind\"].astype('category')\n",
    "outcomes[\"drought_loss_ind\"].astype('category')\n",
    "outcomes['pest_loss_ind'].astype('category')\n",
    "outcomes['animal_loss_ind'].astype('category')\n",
    "outcomes['flood_loss_ind'].astype('category')\n",
    "\n",
    "# Gut-check\n",
    "outcomes.head()\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4870dc-25d2-4eab-8bae-f752633f0150",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helper Functions \n",
    "\n",
    "### 1. Confusion Matrix for Categorical Variables\n",
    "`calculate_confusion_matrix`:\n",
    "To evaluate the performance of our categorical variables, we need to use a confusion matrix instead of r-squared. This function calculates the confusion matrix for binary classification problems based on the given true labels (`y_true`), predicted values (`y_pred`), and a decision boundary (`decision_boundary`) to assign a class to the binary target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ac3bf3-e040-420d-8290-7c1aa96c7f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function to calculate confusion matrix for categorical variables\n",
    "def calculate_confusion_matrix(y_true, y_pred, decision_boundary):\n",
    "    y_pred_adj = np.where(y_pred >= decision_boundary, 1, 0)\n",
    "    cm = confusion_matrix(y_true, y_pred_adj)\n",
    "    if cm.shape == (1, 1):\n",
    "        if y_true.iloc[0] == 0:\n",
    "            tn, fp, fn, tp = cm[0, 0], 0, 0, 0\n",
    "        else:\n",
    "            tn, fp, fn, tp = 0, 0, 0, cm[0, 0]\n",
    "    elif cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        print(\"Unexpected confusion matrix:\")\n",
    "        print(cm)\n",
    "        raise ValueError('Unexpected confusion matrix shape.')\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a98269-73a5-4f7f-a16d-275023df1c3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Custom Function for Block Sampling \n",
    "\n",
    "Here, we define a helper function to block sample across our training and testing sets on our unique spatial identifier (sea_unq). `n_seas_val` and `n_seas_test` in our helper function allow us to specify how many values of `sea_unq` to hold out for our validation and test set. Block sampling allows us to evaluate model performance on unseen spatial areas by holding out all observations of a specified number of unique values in the (`sea_unq`) column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e669989-0911-4f45-b4ae-42281dc8d57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_val_test_indices(sea_ids, n_seas_val, n_seas_to_hold_out_for_test, random_state):\n",
    "    unique_seas = np.unique(sea_ids)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(unique_seas)\n",
    "\n",
    "    # Hold out some SEAs for testing\n",
    "    test_seas = unique_seas[:n_seas_to_hold_out_for_test]\n",
    "    remaining_seas = unique_seas[n_seas_to_hold_out_for_test:]\n",
    "\n",
    "    # Hold out some SEAs for validation\n",
    "    val_seas = remaining_seas[:n_seas_to_hold_out_for_val]\n",
    "    train_seas = remaining_seas[n_seas_to_hold_out_for_val:]\n",
    "\n",
    "    # Convert boolean indices to integer indices\n",
    "    train_indices = np.where(sea_ids.isin(train_seas))[0]\n",
    "    val_indices = np.where(sea_ids.isin(val_seas))[0]\n",
    "    test_indices = np.where(sea_ids.isin(test_seas))[0]\n",
    "\n",
    "    return train_indices, val_indices, test_indices, train_seas, val_seas, test_seas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fd46c0-acb9-4510-9166-f0ecb8b19340",
   "metadata": {},
   "source": [
    "# Modeling Agricultural Variables using different sampling approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d9ae02-a680-4430-a201-8a3aaba9a1ca",
   "metadata": {},
   "source": [
    "## Description \n",
    "\n",
    "This function is designed to use Cross-Validated Ridge Regression to regress our `features` (encoded/summarised \"featurized\" Sentinel 2 satellite imagery data using [MOSAIKS](https://www.nature.com/articles/s41467-021-24638-z) process) on our `outcomes` (ground-truthed Crop Forecast Survey (CFS) data from the Zambian Ministry of Agriculture). We have labelled data at the Survey Enumeration Area (SEA)/year level for 2015-2022 approximating ~1300 observations before imputation steps. We are interested in using 4 different sampling approaches to evaluate our models' performance:\n",
    "1. **5-fold CV using sklearn's RidgeCV**\n",
    "    - Data is split using `train_test_split` into training and testing sets. The training data is again split into a validation set. A RidgeCV model is trained for each target outcome selected. To choose the penalty coefficient alpha for each model, RidgeCV searches over a logspace of 75 values from $10^{-8}$ to $10^8$. The trained model's performance is evaluated on the validation set. \n",
    "    \n",
    "    - This approach serves as the foundation for our other methods.\n",
    "    \n",
    "2. **Bootstrapping + 5-fold CV (incomplete)**\n",
    "    - In this approach, `n_bootstraps` are resampled with replacement from the training + validation data. For each bootstrap sample, we fit a RidgeCV model using 5-fold CV. Meaning that, for each bootstrap sample, we actually fit 5 RidgeCV models on different parts of the data and validate them on different validation sets. This gives us 5 performance estimates for each bootstrap sample, which we average to get a single performance estimate for that bootstrap sample. The bootstrapping process is run in parallel. We then average the performance of all bootstrap samples, to get a final estimate of model performance. This approach gave us our best results. \n",
    "    \n",
    "    - We've marked this method is incomplete because while we can evaluate model performances, we have yet to ensemble each individual bootstrap model into a single model that can be used to output predictions. We hypothesize that the best approach to this ensemble would be to weight each individual model for each variable by its performance ($R^2$). \n",
    "    \n",
    "3. **Block Sampling + 5-fold CV (incomplete)**\n",
    "    - Block sampling allows us to evaluate model performance on unseen spatial areas by holding out all observations of a specified number of unique values in the unique spatial identifier (here, `sea_unq`) column. This operates similarly to our first approach, the only difference being that instead of randomly selecting observations to train/test/validate on, we're specifying to hold out all observations for `n_seas_val` and `n_seas_test` of distinct values of `sea_unq`. The model can then be evaluated on unseen spatial areas by evaluating on validation/test sets.\n",
    "    \n",
    "    - This was unsurprisingly our worst performing approach. We noticed high variability in performances based on which set of SEAs were trained/tested on. Since our targets have high variability across SEAs (some SEAs much larger than others, in very different agricultural regions, etc), our models were overall poor but tended to perform better on SEAs with representative target values. \n",
    "    - More exploration can be conducted here, and we mark this as incomplete because we again have yet to complete the infrastructure to make predictions.\n",
    "    \n",
    "4. **Bootstrapping, Block Sampling and 5-fold CV (incomplete)**\n",
    "    - Our last approach was intended to combine bootstrapping and block sampling to improve our bootstrapping performance. The idea for this approach would be to block on unique SEAs, then generate bootstrap samples of our training/validation data with the remaining SEAs using our bootstrapping approach. A model would then be fit to each resample using 5-fold CV. \n",
    "    \n",
    "    - This approach was also incomplete. The infrastructure to split the data as we intended is still unfinished, so we have yet to evaluate model performances using this approach. \n",
    "    \n",
    "\n",
    "Before any results are printed, our function prints out several parameters selected by the user to ensure the proper parameters are being employed. The parameters output by the function before the models run are: the target columns, the validation and test sizes, whether or not bootstrapping and block sampling were used, and the random state used. We measured the accuracy of our training, validation, and testing sets primarily using the $R^2$ metric. Below are the arguments passed to our function (in a dictionary). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f06c3f8-ca9a-4662-92ee-d896e2fc984b",
   "metadata": {},
   "source": [
    "### Arguments\n",
    "\n",
    "`args`: This is a dictionary that contains all the arguments that are necessary to run the function train_and_evaluate_models.\n",
    "\n",
    "1. `target_columns` (list of strings): contains the names of the columns in the data that are considered as the target variables in the model training process.\n",
    "2. `test_size` (float, optional): represents the proportion of the data to include in the test split. The default value is 0.1, meaning that 10% of the data will be used for testing.\n",
    "3. `categorical_columns` (list of strings): contains the names of the columns in the data that are categorical variables.\n",
    "4. `decision_boundaries` (list of floats): defines the decision boundaries for each categorical target variable. \n",
    "5. `sea_ids` (list of integers): contains the unique spatial identifiers of each Survey Enumeration Area (SEA).\n",
    "6. `validation_size` (float, optional): represents the proportion of the data to include in the validation split. The default value is 0.1, meaning that 10% of the data will be used for validation.\n",
    "7. `bootstrap` (boolean, optional): indicates whether to use bootstrap sampling in the model training process. The default value is False, meaning that bootstrap sampling is not used by default.\n",
    "8. `n_bootstraps` (integer, optional): specifies the number of bootstrap samples to use if bootstrap sampling is enabled. It only has an effect if bootstrap is True.\n",
    "9. `block_sample` (boolean, optional): indicates whether to use block sampling for splitting the data into training, validation, and testing datasets. The default value is False.\n",
    "10. `random_state` (integer, optional): seed used by the random number generator. Setting this value ensures that the splits that we generate are reproducible. The default value is 1.\n",
    "11. `n_seas_held_out_val` (integer, optional): specifies the number of Survey Enumeration Areas (SEAs) to hold out for validation. This is only relevant when using block sampling. The default value is 30.\n",
    "12. `n_seas_held_out_test` (integer, optional): specifies the number of Survey Enumeration Areas (SEAs) to hold out for testing. This is only relevant when using block sampling. The default value is \n",
    "13. `features` : Dataframe containing the features we defined at the top of the notebook\n",
    "14. `outcomes`: Dataframe containing the outcomes we defined at the top of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "103c6d0b-6ef4-45cf-a477-748618b6d4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the arguments as a dictionary\n",
    "args = {\n",
    "    'target_columns': ['frac_area_harv', 'log_maize'], \n",
    "    'test_size': 0.1,\n",
    "    'categorical_columns':[],\n",
    "    'decision_boundaries': [0.5],\n",
    "    'sea_ids': grouped_features['sea_unq'],\n",
    "    'validation_size' : 0.1,\n",
    "    'bootstrap' : True,\n",
    "    'n_bootstraps': 10,\n",
    "    'block_sample': False,\n",
    "    'n_seas_held_out_val': None,\n",
    "    'n_seas_held_out_test': None,\n",
    "    'random_state': 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a5cab05-0deb-48b4-be47-b12c73bbca52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(args):\n",
    "    \"\"\"\n",
    "    This function accepts an `args` dictionary and performs training and evaluation of RidgeCV regression models. \n",
    "    It handles different sampling methods: bootstrap and block sampling and calculates various metrics to evaluate the performance of the models.\n",
    "    \n",
    "    Args:\n",
    "        args (dict): dictionary containing different parameters needed for model training and evaluation.\n",
    "    Returns:\n",
    "        Returns performance metrics for models. \n",
    "    \"\"\"\n",
    "\n",
    "    # Extract values from the args dictionary\n",
    "    target_columns = args['target_columns']  # The target columns in the dataframe\n",
    "    test_size = args.get('test_size', 0.1)  # The proportion of the dataset to include in the test set\n",
    "    categorical_columns = args['categorical_columns']  # The categorical columns in the dataframe\n",
    "    decision_boundaries = args['decision_boundaries']  # The decision boundaries to use when evaluating models\n",
    "    sea_ids = args['sea_ids']  # The unique spatial IDs\n",
    "    validation_size = args.get('validation_size', 0.1)  # The proportion of the training set to include in the validation set\n",
    "    bootstrap = args.get('bootstrap', False)  # Whether to bootstrap the data or not\n",
    "    n_bootstraps = args.get('n_bootstraps', 0)  # The number of bootstrap samples to create\n",
    "    block_sample = args.get('block_sample', False)  # Whether to use block sampling or not\n",
    "    random_state = args.get('random_state', 1)  # The seed for the random number generator\n",
    "    n_seas_held_out_val = args.get('n_seas_held_out_val', 30)  # The number of seas to hold out for validation\n",
    "    n_seas_held_out_test = args.get('n_seas_held_out_test', 10)  # The number of seas to hold out for testing\n",
    "\n",
    "    # Read in data\n",
    "    path = \"/capstone/mosaiks/repos/modeling/data/model_directory/SEA_averaged_features_simple_impute_mean.csv\" ## Your path here\n",
    "    grouped_features = pd.read_csv(path)\n",
    "    features = grouped_features.iloc[:,2:12002] # adjust to select all feature columns\n",
    "    outcomes = grouped_features.iloc[:,12003:12041] #adjust to select all target/outcome variables\n",
    "\n",
    "    # Initialize empty dataframes to store metrics and predictions\n",
    "    predictions_df = pd.DataFrame()\n",
    "    metrics_df = pd.DataFrame(columns=['target_column', 'train_score', 'val_score', 'pearson_coeff', 'fpr', 'roc_auc'])\n",
    "    \n",
    "    # Print out our parameters\n",
    "    print(f\"\\nRunning model with the following parameters:\")\n",
    "    print(f\"Target columns: {target_columns}\")\n",
    "    print(f\"Test size: {test_size}\", f\"Validation size: {validation_size}\")\n",
    "    print(f\"Bootstrap: {bootstrap}\")\n",
    "    print(f\"Random State: {random_state}\")\n",
    "    if bootstrap:\n",
    "        print(f\"Number of bootstrapped samples: {n_bootstraps}\")\n",
    "    print(f\"Block sample: {block_sample}\\n\")\n",
    "    if block_sample:\n",
    "        train_indices, val_indices, test_indices, train_seas, val_seas, test_seas = get_train_val_test_indices(sea_ids, n_seas_to_hold_out_for_val=n_seas_held_out_val, n_seas_to_hold_out_for_test=n_seas_held_out_test, random_state=random_state)\n",
    "        print(f\"Number of seas held out for validation: {n_seas_held_out_val}\\n\")\n",
    "        print(f\"Training SEAs: {train_seas}\")\n",
    "        print(f\"Validation SEAs: {val_seas}\")\n",
    "        print(f\"Testing SEAs: {test_seas}\")\n",
    "        \n",
    "    \n",
    "    ## Body to generate models for each target column\n",
    "    for target_column in target_columns: \n",
    "        # Block Sampling + Bootstrapping switch for sampling methods\n",
    "        if bootstrap and block_sample:\n",
    "            X_train, X_test = features.iloc[train_indices], features.iloc[test_indices]\n",
    "            y_train, y_test = outcomes[target_column].iloc[train_indices], outcomes[target_column].iloc[test_indices]\n",
    "        # Bootstrap sampling method switch    \n",
    "        elif bootstrap:\n",
    "            # Split the data into training and test sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, outcomes[target_column], test_size=test_size, random_state=random_state)\n",
    "        # Block sampling method switch\n",
    "        elif block_sample:\n",
    "            # Use the already obtained train_indices, val_indices, and test_indices\n",
    "            # Create train, validation and test sets using the indices\n",
    "            X_train, X_val, X_test = features.iloc[train_indices], features.iloc[val_indices], features.iloc[test_indices]\n",
    "            y_train, y_val, y_test = outcomes[target_column].iloc[train_indices], outcomes[target_column].iloc[val_indices], outcomes[target_column].iloc[test_indices]\n",
    "        # else just use regular CV     \n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, outcomes[target_column], test_size=test_size, random_state = random_state)\n",
    "            # Split the training data again to create a validation set\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state = random_state)\n",
    "        sea_ids_train = sea_ids[X_train.index]  # Update the sea_ids for the training set\n",
    "        sea_ids_test = sea_ids[X_test.index]  \n",
    "        \n",
    "        # Nested function for bootstrap samples\n",
    "        if bootstrap:\n",
    "            # Set the random seed for reproducibility\n",
    "            rng = np.random.default_rng(random_state)\n",
    "            # Define function for each bootstrap iteration\n",
    "            def bootstrap_iteration(i):\n",
    "                # Create a unique seed for this bootstrap iteration\n",
    "                seed = random_state + i\n",
    "                # Initialize the random number generator with the unique seed\n",
    "                rng = np.random.default_rng(seed)\n",
    "                # Create a bootstrapped training dataset\n",
    "                indices = rng.choice(len(X_train), size=len(X_train), replace=True)\n",
    "                X_train_bootstrap = X_train.iloc[indices]\n",
    "                y_train_bootstrap = y_train.iloc[indices]\n",
    "                X_train_bootstrap, X_val_bootstrap, y_train_bootstrap, y_val_bootstrap = train_test_split(X_train_bootstrap, y_train_bootstrap, test_size=validation_size, random_state=random_state)\n",
    "                # To ensure bootstraps are created properly run the following two lines of code:\n",
    "               # print(f\"Bootstrap sample {i}:\")\n",
    "               # print(pd.concat([X_train_bootstrap, y_train_bootstrap], axis=1).head())\n",
    "                \n",
    "                # Fit the model to the bootstrapped data\n",
    "                cv = 5\n",
    "                ridge_cv = RidgeCV(cv=cv, alphas=np.logspace(-8, 8, base=10, num=75))\n",
    "                ridge_cv.fit(X_train_bootstrap, y_train_bootstrap)\n",
    "\n",
    "                # Store the coefficients and validation scores\n",
    "                train_score = ridge_cv.score(X_train_bootstrap, y_train_bootstrap)\n",
    "                val_score = r2_score(y_val_bootstrap, ridge_cv.predict(X_val_bootstrap))\n",
    "                pearson_coeff, _ = pearsonr(y_val_bootstrap, ridge_cv.predict(X_val_bootstrap)) \n",
    "                \n",
    "                # Calculate false positive rate and AUC-ROC if the target variable is categorical\n",
    "                if target_column in categorical_columns:\n",
    "                    y_val_pred = ridge_cv.predict(X_val_bootstrap)\n",
    "                    fpr = 0\n",
    "                    auc_roc = 0\n",
    "                    for decision_boundary in decision_boundaries:\n",
    "                        # Calculate confusion matrix\n",
    "                        tn, fp, fn, tp = calculate_confusion_matrix(y_val_bootstrap, y_val_pred, decision_boundary)\n",
    "                        # Calculate the false positive rate\n",
    "                        fpr += fp / (fp + tn)\n",
    "                        # Calculate AUC-ROC\n",
    "                        auc_roc += roc_auc_score(y_val_bootstrap, y_val_pred)\n",
    "                    fpr /= len(decision_boundaries)  # Get average false positive rate\n",
    "                    auc_roc /= len(decision_boundaries)  # Get average AUC-ROC\n",
    "                    return ridge_cv.coef_, train_score, val_score, pearson_coeff, fpr, auc_roc\n",
    "                else:\n",
    "                    return ridge_cv.coef_, train_score, val_score, pearson_coeff, None, None\n",
    "\n",
    "            # Run bootstrap iterations in parallel\n",
    "            results = Parallel(n_jobs=-1)(delayed(bootstrap_iteration)(i) for i in range(n_bootstraps))\n",
    "            # Unpack results\n",
    "            coefs, train_scores, val_scores, pearson_coeffs, false_positive_rates, auc_rocs = zip(*results)\n",
    "            # Calculate the average coefficients and validation scores\n",
    "            avg_coefs = np.mean(coefs, axis=0)\n",
    "            avg_train_score = np.mean(train_scores)\n",
    "            avg_val_score = np.mean(val_scores)\n",
    "            avg_pearson_coeff = np.mean(pearson_coeffs)\n",
    "            \n",
    "                        # Calculate the average false positive rate and AUC-ROC for categorical variables\n",
    "            if target_column in categorical_columns:\n",
    "                avg_false_positive_rate = np.nanmean(false_positive_rates)\n",
    "                avg_auc_roc = np.nanmean(auc_rocs)\n",
    "                print(f\"Average false positive rate: {avg_false_positive_rate:0.2f}\")\n",
    "                print(f\"Average AUC-ROC: {avg_auc_roc:0.2f}\")\n",
    "                \n",
    "            print(f\"Target variable: {target_column}\")\n",
    "            print(f\"Average training R2 score: {avg_train_score:0.2f}\")\n",
    "            print(f\"Average validation R2 score: {avg_val_score:0.2f}\")\n",
    "            print(f\"Average Pearson's correlation coefficient: {avg_pearson_coeff:0.2f}\")\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            cv = 5\n",
    "            ridge_cv = RidgeCV(cv=cv, alphas=np.logspace(-8, 8, base=10, num=75))\n",
    "            ridge_cv.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions on the validation data\n",
    "            y_val_pred = ridge_cv.predict(X_val)\n",
    "            # Update the predictions DataFrame with the new predictions\n",
    "            predictions_df[target_column] = y_val_pred\n",
    "            \n",
    "            if target_column in categorical_columns:\n",
    "                for decision_boundary in decision_boundaries:\n",
    "                    # Calculate confusion matrix\n",
    "                    tn, fp, fn, tp = calculate_confusion_matrix(y_val, y_val_pred, decision_boundary)\n",
    "                # Calculate the false positive rate\n",
    "                    false_positive_rate = fp / (fp + tn)\n",
    "                # Calculate AUC-ROC\n",
    "                    auc_roc = roc_auc_score(y_val, y_val_pred)\n",
    "                    print(f\"Target variable: {target_column} (Categorical)\")\n",
    "                    print(f\"Decision boundary: {decision_boundary}\")\n",
    "                    print(f\"False positive rate: {false_positive_rate:0.2f}\")\n",
    "                    print(f\"AUC-ROC: {auc_roc:0.2f}\")\n",
    "                    print()\n",
    "                    \n",
    "                metrics_df = metrics_df.append({\n",
    "                'target_column': target_column,\n",
    "                'fpr': false_positive_rate,\n",
    "                'roc_auc': auc_roc}, ignore_index=True)\n",
    "                \n",
    "            else:\n",
    "                # Calculate Pearson's correlation coefficient\n",
    "                pearson_coeff, _ = pearsonr(y_val, y_val_pred)\n",
    "                # Calculate training R squared\n",
    "                train_r_squared = ridge_cv.score(X_train, y_train)\n",
    "                # Calculate validation R squared\n",
    "                val_r_squared = ridge_cv.score(X_val, y_val)\n",
    "                metrics_df = metrics_df.append({\n",
    "                'target_column': target_column,\n",
    "                'train_score': train_r_squared,\n",
    "                'val_score': val_r_squared,\n",
    "                'pearson_coeff': pearson_coeff}, ignore_index=True)\n",
    "                    \n",
    "                print()\n",
    "                print(f\"Target variable: {target_column}\")\n",
    "                print(f\"Estimated regularization parameter: {ridge_cv.alpha_}\")\n",
    "                print(f\"Training R2 performance: {train_r_squared:0.2f}\")\n",
    "                print(f\"Validation R2 performance: {val_r_squared:0.2f}\")\n",
    "                print(f\"Pearson's correlation coefficient: {pearson_coeff:0.2f}\")\n",
    "                print()\n",
    "\n",
    "    return predictions_df, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09888b96-77c7-4611-8f2b-baba21ace8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with the following parameters:\n",
      "Target columns: ['frac_area_harv', 'log_maize']\n",
      "Test size: 0.1 Validation size: 0.1\n",
      "Bootstrap: True\n",
      "Random State: 50\n",
      "Number of bootstrapped samples: 10\n",
      "Block sample: False\n",
      "\n",
      "Target variable: frac_area_harv\n",
      "Average training R2 score: 0.77\n",
      "Average validation R2 score: 0.54\n",
      "Average Pearson's correlation coefficient: 0.74\n",
      "\n",
      "Target variable: log_maize\n",
      "Average training R2 score: 0.90\n",
      "Average validation R2 score: 0.72\n",
      "Average Pearson's correlation coefficient: 0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_df = train_and_evaluate_models(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a606e-63ff-4508-87b4-653b388bfbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mosaiks-modeling",
   "language": "python",
   "name": "mosaiks-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
