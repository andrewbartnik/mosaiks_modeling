{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe56655",
   "metadata": {},
   "source": [
    "# Predicting Agricultural Variables\n",
    "\n",
    "This notebook contains the modeling approach used by the 2023 MOSAIKS team as part of the University of California, Santa Barbara's Bren School of the Environment & Management Masters of Environmental Data Science (MEDS) Program's Capstone Project. \n",
    "\n",
    "This notebook immediately follows the [feature preprocessing](https://github.com/mosaiks-capstone/Modeling/blob/main/feature_preprocessing.ipynb) notebook used to aggregate and join featurized satellite imagery (generated [here](https://github.com/mosaiks-capstone/Featurization)) and ground truth data (in our case, Crop Forecast Survey (CFS) Data collected by the Zambian Ministry of Agriculture). This notebook regresses our features on our outcomes, and generates predictions for each. As of now, this notebook only supports generating predictions for our 5-fold RidgeCV model approach\n",
    "\n",
    "To evaluate model performances using different sampling methods, please see the [model_performance_eval](https://github.com/mosaiks-capstone/Modeling/blob/main/model_performance_eval.ipynb) notebook.\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b4f19f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import geopandas as gpd\n",
    "import pyarrow\n",
    "\n",
    "from IPython.display import display\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.axes import Axes\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, r2_score, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.linalg import LinAlgWarning\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import check_random_state, resample\n",
    "\n",
    "\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0591f-583f-4945-b1d2-0323ae715531",
   "metadata": {},
   "source": [
    "# Read in Data\n",
    "\n",
    "We first read in the aggregated features and ground-truth data joined in  `feature_preprocessing.ipynb` .\n",
    "\n",
    "The joined data being read in should take on the following form:\n",
    "\n",
    "| spatial_identifier | year | target_1 | target_2 | feature1| feature2 | feature3\n",
    "| ----| ----| ---- | ---- | -- | -- | -- |\n",
    "| 1   | 2016 | 72 | 13 | 1.23 | 3.25 | 0.123\n",
    "| 2   | 2016  | 50 | 7.5 | 0.78| 1.2 | 2.4\n",
    "\n",
    "\n",
    "\n",
    "In our case, our unique spatial_identifiers are `sea_unq`. This enables us to regress `target_1` and `target_2` on our features, using the following equation:\n",
    "\n",
    "$y_{1}$ = $\\beta_{1}$$x_{1}$ + $\\beta_{2}$$x_{2}$ + $\\beta_{3}$$x_{3}$ + $\\beta_{n}$$x_{n}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f593b6f-5740-41de-b785-3a4555428899",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>sea_unq</th>\n",
       "      <th>0_1</th>\n",
       "      <th>0_2</th>\n",
       "      <th>0_3</th>\n",
       "      <th>0_4</th>\n",
       "      <th>0_5</th>\n",
       "      <th>0_6</th>\n",
       "      <th>0_7</th>\n",
       "      <th>0_8</th>\n",
       "      <th>...</th>\n",
       "      <th>log_sweetpotatoes</th>\n",
       "      <th>log_groundnuts</th>\n",
       "      <th>log_soybeans</th>\n",
       "      <th>loss_ind</th>\n",
       "      <th>drought_loss_ind</th>\n",
       "      <th>flood_loss_ind</th>\n",
       "      <th>animal_loss_ind</th>\n",
       "      <th>pest_loss_ind</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.157999e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>5.935403</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.659357</td>\n",
       "      <td>27.807993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>6.299240e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>5.935403</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.493902</td>\n",
       "      <td>27.959205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.008277e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689155</td>\n",
       "      <td>5.935403</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.772690</td>\n",
       "      <td>28.634660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.590917e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>-1.408767</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.905428</td>\n",
       "      <td>27.406446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.113844e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.525729</td>\n",
       "      <td>3.354421</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.962298</td>\n",
       "      <td>27.381719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>388</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>8.372727e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>9.367183</td>\n",
       "      <td>8.098897</td>\n",
       "      <td>7.336848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.652084</td>\n",
       "      <td>25.116478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>389</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>7.513932e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>8.048788</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.966394</td>\n",
       "      <td>22.794290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>390</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>9.321970e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.863267</td>\n",
       "      <td>8.154788</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.240607</td>\n",
       "      <td>23.101535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>391</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>1.540414e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>8.065208</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.485957</td>\n",
       "      <td>24.338360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>2021.0</td>\n",
       "      <td>392</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>2.518747e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>6.364023</td>\n",
       "      <td>7.960925</td>\n",
       "      <td>6.565149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-16.286023</td>\n",
       "      <td>23.242993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1369 rows × 12043 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  sea_unq       0_1       0_2       0_3       0_4       0_5  \\\n",
       "0     2016.0        1  0.000000  0.000863  0.000783  0.000000  0.000000   \n",
       "1     2016.0        2  0.000069  0.000863  0.000783  0.000000  0.000002   \n",
       "2     2016.0        7  0.001141  0.000863  0.000783  0.000329  0.000000   \n",
       "3     2016.0        9  0.001131  0.000863  0.000783  0.000006  0.000004   \n",
       "4     2016.0       10  0.001131  0.000863  0.000783  0.000000  0.000000   \n",
       "...      ...      ...       ...       ...       ...       ...       ...   \n",
       "1364  2021.0      388  0.001131  0.000076  0.000106  0.000075  0.000046   \n",
       "1365  2021.0      389  0.001131  0.000863  0.000018  0.000104  0.000202   \n",
       "1366  2021.0      390  0.000821  0.000345  0.000180  0.000227  0.000306   \n",
       "1367  2021.0      391  0.001131  0.000863  0.000353  0.000323  0.000244   \n",
       "1368  2021.0      392  0.001661  0.001136  0.000874  0.000720  0.000429   \n",
       "\n",
       "           0_6       0_7           0_8  ...  log_sweetpotatoes  \\\n",
       "0     0.000000  0.000000  6.157999e-06  ...           6.364023   \n",
       "1     0.000014  0.000047  6.299240e-05  ...           6.364023   \n",
       "2     0.000000  0.000000  1.008277e-03  ...           0.689155   \n",
       "3     0.000010  0.000014  2.590917e-05  ...           6.364023   \n",
       "4     0.000000  0.000000  3.113844e-07  ...           2.525729   \n",
       "...        ...       ...           ...  ...                ...   \n",
       "1364  0.000112  0.000144  8.372727e-04  ...           9.367183   \n",
       "1365  0.000424  0.000395  7.513932e-04  ...           6.364023   \n",
       "1366  0.000457  0.000474  9.321970e-04  ...           7.863267   \n",
       "1367  0.000222  0.000311  1.540414e-03  ...           6.364023   \n",
       "1368  0.000714  0.001367  2.518747e-03  ...           6.364023   \n",
       "\n",
       "      log_groundnuts  log_soybeans  loss_ind  drought_loss_ind  \\\n",
       "0           5.935403      6.565149       0.0               0.0   \n",
       "1           5.935403      6.565149       0.0               0.0   \n",
       "2           5.935403      6.565149       1.0               1.0   \n",
       "3          -1.408767      6.565149       1.0               0.0   \n",
       "4           3.354421      6.565149       1.0               0.0   \n",
       "...              ...           ...       ...               ...   \n",
       "1364        8.098897      7.336848       0.0               0.0   \n",
       "1365        8.048788      6.565149       1.0               1.0   \n",
       "1366        8.154788      6.565149       1.0               0.0   \n",
       "1367        8.065208      6.565149       1.0               0.0   \n",
       "1368        7.960925      6.565149       1.0               0.0   \n",
       "\n",
       "      flood_loss_ind  animal_loss_ind  pest_loss_ind        lat        lon  \n",
       "0                0.0              0.0            0.0 -13.659357  27.807993  \n",
       "1                0.0              0.0            0.0 -13.493902  27.959205  \n",
       "2                0.0              0.0            0.0 -13.772690  28.634660  \n",
       "3                0.0              0.0            0.0 -12.905428  27.406446  \n",
       "4                0.0              0.0            0.0 -12.962298  27.381719  \n",
       "...              ...              ...            ...        ...        ...  \n",
       "1364             0.0              0.0            0.0 -14.652084  25.116478  \n",
       "1365             0.0              0.0            0.0 -13.966394  22.794290  \n",
       "1366             0.0              0.0            0.0 -14.240607  23.101535  \n",
       "1367             1.0              0.0            0.0 -16.485957  24.338360  \n",
       "1368             0.0              0.0            0.0 -16.286023  23.242993  \n",
       "\n",
       "[1369 rows x 12043 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Insert path to joined ground data + features\n",
    "path = \"/capstone/mosaiks/repos/modeling/data/model_directory/SEA_averaged_features_simple_impute_mean.csv\" ## Your path here\n",
    "\n",
    "grouped_features = pd.read_csv(path)\n",
    "grouped_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16ec43-9d03-4c59-a6b2-8fb2947c5a81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select Features and Outcomes\n",
    "\n",
    "We then select all observations for each of the columns containing the features. We do the same with our outcome/target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0640f9c-6682-4d85-866c-f7d2be3135cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_1</th>\n",
       "      <th>0_2</th>\n",
       "      <th>0_3</th>\n",
       "      <th>0_4</th>\n",
       "      <th>0_5</th>\n",
       "      <th>0_6</th>\n",
       "      <th>0_7</th>\n",
       "      <th>0_8</th>\n",
       "      <th>0_9</th>\n",
       "      <th>0_10</th>\n",
       "      <th>...</th>\n",
       "      <th>999_3</th>\n",
       "      <th>999_4</th>\n",
       "      <th>999_5</th>\n",
       "      <th>999_6</th>\n",
       "      <th>999_7</th>\n",
       "      <th>999_8</th>\n",
       "      <th>999_9</th>\n",
       "      <th>999_10</th>\n",
       "      <th>999_11</th>\n",
       "      <th>999_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.157999e-06</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.274676</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115388</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>6.299240e-05</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>0.939709</td>\n",
       "      <td>0.049106</td>\n",
       "      <td>0.039969</td>\n",
       "      <td>0.004752</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.071531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.008277e-03</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.071531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2.590917e-05</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>0.006391</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.071531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.113844e-07</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060421</td>\n",
       "      <td>0.005570</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>0.003991</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.001435</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>0.071531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0_1       0_2       0_3       0_4       0_5       0_6       0_7  \\\n",
       "0  0.000000  0.000863  0.000783  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000069  0.000863  0.000783  0.000000  0.000002  0.000014  0.000047   \n",
       "2  0.001141  0.000863  0.000783  0.000329  0.000000  0.000000  0.000000   \n",
       "3  0.001131  0.000863  0.000783  0.000006  0.000004  0.000010  0.000014   \n",
       "4  0.001131  0.000863  0.000783  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "            0_8       0_9      0_10  ...     999_3     999_4     999_5  \\\n",
       "0  6.157999e-06  0.000207  0.001568  ...  0.060421  1.000000  0.274676   \n",
       "1  6.299240e-05  0.000168  0.001568  ...  0.060421  0.939709  0.049106   \n",
       "2  1.008277e-03  0.001360  0.002211  ...  0.060421  0.006789  1.000000   \n",
       "3  2.590917e-05  0.000110  0.001568  ...  0.060421  0.005561  0.006391   \n",
       "4  3.113844e-07  0.000012  0.001568  ...  0.060421  0.005570  0.006739   \n",
       "\n",
       "      999_6     999_7     999_8     999_9    999_10    999_11    999_12  \n",
       "0  1.000000  0.115388  0.002708  0.001319  0.002867  0.003866  1.000000  \n",
       "1  0.039969  0.004752  0.002671  0.002439  0.002867  0.003866  0.071531  \n",
       "2  1.000000  1.000000  0.000517  0.000343  0.000396  0.003866  0.071531  \n",
       "3  0.004212  0.003235  0.001937  0.001683  0.002867  0.003866  0.071531  \n",
       "4  0.003991  0.002857  0.001979  0.001435  0.002867  0.003866  0.071531  \n",
       "\n",
       "[5 rows x 12000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features and outcomes:\n",
    "features = grouped_features.iloc[:,2:12002] # adjust to select all feature columns\n",
    "outcomes = grouped_features.iloc[:,12003:12041] #adjust to select all target/outcome variables\n",
    "\n",
    "## Ensure each of the binary target variables are of categorical data types\n",
    "outcomes[\"loss_ind\"].astype('category')\n",
    "outcomes[\"drought_loss_ind\"].astype('category')\n",
    "outcomes['pest_loss_ind'].astype('category')\n",
    "outcomes['animal_loss_ind'].astype('category')\n",
    "outcomes['flood_loss_ind'].astype('category')\n",
    "\n",
    "# Gut-check\n",
    "outcomes.head()\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf61531a-76d4-46b6-b1b6-5a6cf9bf5508",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helper function \n",
    "`calculate_confusion_matrix`:\n",
    "To evaluate the performance of our categorical variables, we need to use a confusion matrix instead of r-squared. This function calculates the confusion matrix for binary classification problems based on the given true labels (`y_true`), predicted values (`y_pred`), and a decision boundary (`decision_boundary`) to assign a class to the binary target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5ac3bf3-e040-420d-8290-7c1aa96c7f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(y_true, y_pred, decision_boundary):\n",
    "    y_pred_adj = np.where(y_pred >= decision_boundary, 1, 0)\n",
    "    cm = confusion_matrix(y_true, y_pred_adj)\n",
    "    if cm.shape == (1, 1):\n",
    "        if y_true.iloc[0] == 0:\n",
    "            tn, fp, fn, tp = cm[0, 0], 0, 0, 0\n",
    "        else:\n",
    "            tn, fp, fn, tp = 0, 0, 0, cm[0, 0]\n",
    "    elif cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        print(\"Unexpected confusion matrix:\")\n",
    "        print(cm)\n",
    "        raise ValueError('Unexpected confusion matrix shape.')\n",
    "    return tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc11038",
   "metadata": {},
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654341b7-0c26-4701-b9a1-33c691208ba0",
   "metadata": {},
   "source": [
    "## Description \n",
    "\n",
    "This function is designed to use Cross-Validated Ridge Regression to regress our `features` (encoded/summarised \"featurized\" Sentinel 2 satellite imagery data using [MOSAIKS](https://www.nature.com/articles/s41467-021-24638-z) process) on our `outcomes` (ground-truthed Crop Forecast Survey (CFS) data from the Zambian Ministry of Agriculture). We have labelled data at the Survey Enumeration Area (SEA)/year level for 2015-2022 approximating ~1300 observations before imputation steps.\n",
    "\n",
    "The approach employs **5-fold CV using sklearn's RidgeCV**\n",
    "    - Data is split using `train_test_split` into training and testing sets. The training data is again split into a validation set. A RidgeCV model is trained for each target outcome selected. To choose the penalty coefficient alpha for each model, RidgeCV searches over a logspace of 75 values from $10^{-8}$ to $10^8$. The trained model's performance is evaluated on the validation set. \n",
    "    \n",
    "\n",
    "Before any results are printed, our function prints out several parameters selected by the user to ensure the proper parameters are being employed. We measured the accuracy of our training, validation, and testing sets primarily using the $R^2$ metric. Below are the arguments passed to our function (in a dictionary). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce0d56-18c7-4ab2-98ca-de1c3ccc9217",
   "metadata": {},
   "source": [
    "### Arguments\n",
    "\n",
    "`args`: This is a dictionary that contains all the arguments that are necessary to run the function train_and_evaluate_models.\n",
    "\n",
    "1. `target_columns` (list of strings): contains the names of the columns in the data that are considered as the target variables in the model training process.\n",
    "2. `test_size` (float, optional): represents the proportion of the data to include in the test split. The default value is 0.1, meaning that 10% of the data will be used for testing.\n",
    "3. `categorical_columns` (list of strings): contains the names of the columns in the data that are categorical variables.\n",
    "4. `decision_boundaries` (list of floats): defines the decision boundaries for each categorical target variable. \n",
    "5. `sea_ids` (list of integers): contains the unique spatial identifiers of each Survey Enumeration Area (SEA).\n",
    "6. `validation_size` (float, optional): represents the proportion of the data to include in the validation split. The default value is 0.1, meaning that 10% of the data will be used for validation.\n",
    "7.  `random_state` (integer, optional): seed used by the random number generator. Setting this value ensures that the splits that we generate are reproducible. The default value is 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "103c6d0b-6ef4-45cf-a477-748618b6d4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the arguments as a dictionary\n",
    "args = {\n",
    "    'target_columns': outcomes.columns,\n",
    "    'test_size': 0.1,\n",
    "    'categorical_columns':['loss_ind','drought_loss_ind', 'flood_loss_ind','animal_loss_ind','pest_loss_ind'],\n",
    "    'decision_boundaries': [0.3,0.5,0.7],\n",
    "    'sea_ids': grouped_features['sea_unq'],\n",
    "    'validation_size' : 0.1,\n",
    "    'random_state': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c626605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(args):\n",
    "    # Extracting input parameters\n",
    "    target_columns = args['target_columns']\n",
    "    test_size = args.get('test_size', 0.1)\n",
    "    categorical_columns = args['categorical_columns']\n",
    "    decision_boundaries = args['decision_boundaries']\n",
    "    sea_ids = args['sea_ids']\n",
    "    validation_size = args.get('validation_size', 0.1)\n",
    "    random_state = args.get('random_state', False)\n",
    "    \n",
    "    # Read the grouped features from a CSV file\n",
    "    grouped_features = pd.read_csv(\"/capstone/mosaiks/repos/modeling/data/model_directory/SEA_averaged_features_manual_impute_bfill_modeltrain.csv\")\n",
    "\n",
    "    # Extract the relevant features, outcomes, and year columns\n",
    "    features = grouped_features.iloc[:, 5:12005]\n",
    "    outcomes = grouped_features.iloc[:, 12006:]\n",
    "    year = grouped_features.iloc[:, 0]\n",
    "    \n",
    "    # Initialize data structures to store metrics and results\n",
    "    metrics_df = pd.DataFrame(columns=['target_column', 'train_score', 'val_score', 'pearson_coeff'])\n",
    "    models = {}\n",
    "    X_trains = {}\n",
    "    X_tests = {}\n",
    "    y_trains = pd.DataFrame()\n",
    "    y_tests = pd.DataFrame()\n",
    "    y_year = pd.DataFrame()\n",
    "    \n",
    "    # Print the model parameters\n",
    "    print(f\"\\nRunning model with the following parameters:\")\n",
    "    print(f\"Target columns: {target_columns}\")\n",
    "    print(f\"Test size: {test_size}\", f\"Validation size: {validation_size}\")\n",
    "    print(f\"Random State: {random_state}\")\n",
    "\n",
    "    # Iterate over each target column\n",
    "    for target_column in target_columns:\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features, outcomes[target_column], test_size=test_size, random_state = random_state)\n",
    "        \n",
    "        # Split the training data again to create a validation set\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state = random_state)\n",
    "        \n",
    "        # Store the training and testing data for each target column\n",
    "        X_trains[target_column] = X_train\n",
    "        X_tests[target_column] = X_test\n",
    "        y_trains[target_column] = y_train\n",
    "        y_tests[target_column] = y_test\n",
    "        y_year[target_column] = year.loc[y_trains.index]\n",
    "\n",
    "        # Train a RidgeCV model with cross-validation\n",
    "        cv = 5\n",
    "        ridge_cv = RidgeCV(cv=cv, alphas=np.logspace(-8, 8, base=10, num=75))\n",
    "        ridge_cv.fit(X_train, y_train)\n",
    "        \n",
    "        # Store the trained model for each target column\n",
    "        models[target_column] = ridge_cv\n",
    "        \n",
    "        # Make predictions on the training and validation data\n",
    "        y_val_pred = ridge_cv.predict(X_val)\n",
    "        y_train_pred = ridge_cv.predict(X_train)\n",
    "\n",
    "        # Perform evaluation for categorical target columns\n",
    "        if target_column in categorical_columns:\n",
    "            for decision_boundary in decision_boundaries:\n",
    "                # Calculate confusion matrix\n",
    "                tn, fp, fn, tp = calculate_confusion_matrix(y_val, y_val_pred, decision_boundary)\n",
    "\n",
    "                # Calculate the false positive rate\n",
    "                false_positive_rate = fp / (fp + tn)\n",
    "\n",
    "                # Calculate AUC-ROC\n",
    "                auc_roc = roc_auc_score(y_val, y_val_pred)\n",
    "\n",
    "                # Print evaluation metrics for categorical columns\n",
    "                print(f\"Target variable: {target_column} (Categorical)\")\n",
    "                print(f\"Decision boundary: {decision_boundary}\")\n",
    "                print(f\"False positive rate: {false_positive_rate:0.2f}\")\n",
    "                print(f\"AUC-ROC: {auc_roc:0.2f}\")\n",
    "                print()\n",
    "        else:\n",
    "            # Calculate Pearson's correlation coefficient\n",
    "            pearson_coeff, _ = pearsonr(y_val, y_val_pred)\n",
    "\n",
    "            # Calculate training R squared\n",
    "            train_r_squared = ridge_cv.score(X_train, y_train)\n",
    "\n",
    "            # Calculate validation R squared\n",
    "            val_r_squared = ridge_cv.score(X_val, y_val)\n",
    "            \n",
    "            # Append metrics to the metrics DataFrame\n",
    "            metrics_df = metrics_df.append({\n",
    "                'target_column': target_column,\n",
    "                'train_score': train_r_squared,\n",
    "                'val_score': val_r_squared,\n",
    "                'pearson_coeff': pearson_coeff}, ignore_index=True)\n",
    "                \n",
    "            # Print evaluation metrics for non-categorical columns\n",
    "            print()\n",
    "            print(f\"Target variable: {target_column}\")\n",
    "            print(f\"Estimated regularization parameter: {ridge_cv.alpha_}\")\n",
    "            print(f\"Training R2 performance: {train_r_squared:0.2f}\")\n",
    "            print(f\"Validation R2 performance: {val_r_squared:0.2f}\")\n",
    "            print(f\"Pearson's correlation coefficient: {pearson_coeff:0.2f}\")\n",
    "            print()\n",
    "\n",
    "    # Return the collected data and results\n",
    "    return X_trains, X_tests, y_trains, y_tests, metrics_df, models, y_year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6414f7ab-c9a8-4788-be39-7a2762cd58fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model with the following parameters:\n",
      "Target columns: Index(['total_area_harv_ha', 'total_area_lost_ha', 'total_harv_kg',\n",
      "       'yield_kgha', 'frac_area_harv', 'frac_area_loss', 'area_lost_fire',\n",
      "       'maize', 'groundnuts', 'mixed_beans', 'popcorn', 'sorghum', 'soybeans',\n",
      "       'sweet_potatoes', 'bunding', 'monocrop', 'mixture', 'frac_loss_drought',\n",
      "       'frac_loss_flood', 'frac_loss_animal', 'frac_loss_pests',\n",
      "       'frac_loss_soil', 'frac_loss_fert', 'prop_till_plough',\n",
      "       'prop_till_ridge', 'prop_notill', 'prop_hand', 'prop_mono', 'prop_mix',\n",
      "       'log_maize', 'log_sweetpotatoes', 'log_groundnuts', 'log_soybeans',\n",
      "       'loss_ind', 'drought_loss_ind', 'flood_loss_ind', 'animal_loss_ind',\n",
      "       'pest_loss_ind'],\n",
      "      dtype='object')\n",
      "Test size: 0.1 Validation size: 0.1\n",
      "Random State: 50\n"
     ]
    }
   ],
   "source": [
    "X_trains, X_tests, y_trains, y_tests, metrics_df, models, y_year  = train_and_evaluate_models(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30906c",
   "metadata": {},
   "source": [
    "### Train Set\n",
    "\n",
    "After training models for each specified target variable in `target_columns`, we employ these models to create and store predictions and $R^2$ scores for each target column on our training data. Our training data has been aggregated by survey enumeration area (SEA) and year, which means that each of the 436 rows of `y_pred_train` represents a prediction made for a particular SEA during a particular year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eed876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dataframes for storing the predicted values and R2 scores\n",
    "y_pred_train = pd.DataFrame()\n",
    "r2_train = pd.DataFrame()\n",
    "\n",
    "# Iterate over the keys in models dictionary\n",
    "for target_column in models.keys():\n",
    "    # Get the corresponding trained model for the target column\n",
    "    model = models[target_column]\n",
    "    \n",
    "    # Get the training data for the target column\n",
    "    X_train_column = X_trains[target_column]\n",
    "    y_train_column = y_trains[target_column]\n",
    "    \n",
    "    # Make predictions for the target column\n",
    "    y_pred_train_column = np.maximum(model.predict(X_train_column), 0)\n",
    "    \n",
    "    # Compute the R2 score for the target column\n",
    "    r2_train_column = r2_score(y_train_column, y_pred_train_column)\n",
    "    \n",
    "    # Store the predicted values and R2 score in their respective dictionaries\n",
    "    y_pred_train[target_column] = y_pred_train_column\n",
    "    r2_train[target_column] = [r2_train_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca86fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314cafff",
   "metadata": {},
   "source": [
    "### Visualize Performance of Train Set \n",
    "\n",
    "We visualize performances of the training set through scatterplots of our predicted values versus ground-truthed values. These scatterplots include a regression line, and display the R^2 value for the selected variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of variable names from the dataframes\n",
    "variable_names = list(y_pred_train.columns)\n",
    "\n",
    "# Create the dropdown widget\n",
    "variable_dropdown = widgets.Dropdown(options=variable_names, description='Variable:')\n",
    "\n",
    "# create a container widget to hold the dropdown and the plot\n",
    "container = widgets.VBox(children=[variable_dropdown])\n",
    "\n",
    "# Create an output widget to display the plot\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "# Define a function to update the plot based on the selected variable\n",
    "def update_plot_train(variable):\n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        # Create the scatterplot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y_pred_train[variable], y_trains[variable])\n",
    "        ax.axline([0, 0], [1, 1], c=\"k\")\n",
    "\n",
    "        # Extract the R2 value from the r2_train dataframe\n",
    "        r2_value = r2_train[variable]\n",
    "        r2_value = round(r2_value, 2)\n",
    "\n",
    "        # Set the title with the current title as a subtitle and the new title as \"Variable: [variable]\"\n",
    "        sub_title = f\"Model applied to train data n = {len(y_trains)}, R$^2$ = {r2_value}\"\n",
    "        title = f\"Variable: {variable}\"\n",
    "        plt.title(sub_title, fontsize=12, y=1.0, loc='left')\n",
    "        plt.title(title, fontsize=14, y=1.15, loc='center')\n",
    "\n",
    "        # Set x and y axis labels\n",
    "        ax.set_xlabel(\"Predicted\", fontsize=15)\n",
    "        ax.set_ylabel(\"Ground Truth\", fontsize=15)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "# Define a function to update the dropdown options when the variable names change\n",
    "def update_dropdown_options(change):\n",
    "    variable_dropdown.options = variable_names\n",
    "\n",
    "# Call the update_plot_train function with the initial value of the dropdown\n",
    "update_plot_train(variable_dropdown.value)\n",
    "\n",
    "# Register the event handler to update the dropdown options\n",
    "variable_dropdown.observe(update_dropdown_options, 'options')\n",
    "\n",
    "# Set up the interaction between the dropdown and the plot\n",
    "def dropdown_eventhandler(change):\n",
    "    variable = change.new\n",
    "    update_plot_train(variable)\n",
    "\n",
    "variable_dropdown.observe(dropdown_eventhandler, 'value')\n",
    "\n",
    "# Display the dropdown and the plot\n",
    "display(widgets.VBox([variable_dropdown, plot_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878d7f6",
   "metadata": {},
   "source": [
    "### Test Set \n",
    "\n",
    "Next, we employ these models to create and store predictions and R^2 scores for each target column on our testing data. Again, our testing data has been aggregated by survey enumeration area (SEA) and year, which means that each of the 436 rows of `y_pred_test` represents a prediction made for a particular SEA during a particular year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309523bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dictionaries for storing the predicted values and R2 scores\n",
    "y_pred_test = pd.DataFrame()\n",
    "r2_test = pd.DataFrame()\n",
    "\n",
    "# Iterate over the keys in models dictionary\n",
    "for target_column in models.keys():\n",
    "    # Get the corresponding trained model for the target column\n",
    "    model = models[target_column]\n",
    "    \n",
    "    # Get the training data for the target column\n",
    "    X_test_column = X_tests[target_column]\n",
    "    y_test_column = y_tests[target_column]\n",
    "    \n",
    "    # Make predictions for the target column\n",
    "    y_pred_test_column = np.maximum(model.predict(X_test_column), 0)\n",
    "    \n",
    "    # Compute the R2 score for the target column\n",
    "    r2_test_column = r2_score(y_test_column, y_pred_test_column)\n",
    "    \n",
    "    # Store the predicted values and R2 score in their respective dictionaries\n",
    "    y_pred_test[target_column] = y_pred_test_column\n",
    "    r2_test[target_column] = [r2_test_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe58c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a0179",
   "metadata": {},
   "source": [
    "### Visualize Performance of Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60be005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of variable names from the dataframes\n",
    "variable_names = list(y_pred_test.columns)\n",
    "\n",
    "# create a container widget to hold the dropdown and the plot\n",
    "container = widgets.VBox(children=[variable_dropdown])\n",
    "\n",
    "# Create the dropdown widget\n",
    "variable_dropdown = widgets.Dropdown(options=variable_names, description='Variable:')\n",
    "\n",
    "# Create an output widget to display the plot\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "\n",
    "# Define a function to update the plot based on the selected variable\n",
    "def update_plot_test(variable):\n",
    "    with plot_output:\n",
    "        clear_output(wait=True)\n",
    "        # Create the scatterplot\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(y_pred_test[variable], y_tests[variable])\n",
    "        ax.axline([0, 0], [1, 1], c=\"k\")\n",
    "\n",
    "        # Extract the R2 value from the r2_train dataframe\n",
    "        r2_value = r2_test[variable]\n",
    "        r2_value = round(r2_value, 2)\n",
    "\n",
    "        # Set the title with the current title as a subtitle and the new title as \"Variable: [variable]\"\n",
    "        sub_title = f\"Model applied to test data n = {len(y_tests)}, R$^2$ = {r2_value}\"\n",
    "        title = f\"Variable: {variable}\"\n",
    "        plt.title(sub_title, fontsize=12, y=1.0, loc='left')\n",
    "        plt.title(title, fontsize=14, y=1.15, loc='center')\n",
    "\n",
    "        # Set x and y axis labels\n",
    "        ax.set_xlabel(\"Predicted\", fontsize=15)\n",
    "        ax.set_ylabel(\"Ground Truth\", fontsize=15)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "\n",
    "# Define a function to update the dropdown options when the variable names change\n",
    "def update_dropdown_options(change):\n",
    "    variable_dropdown.options = variable_names\n",
    "\n",
    "# Call the update_plot_train function with the initial value of the dropdown\n",
    "update_plot_test(variable_dropdown.value)\n",
    "\n",
    "# Register the event handler to update the dropdown options\n",
    "variable_dropdown.observe(update_dropdown_options, 'options')\n",
    "\n",
    "# Set up the interaction between the dropdown and the plot\n",
    "def dropdown_eventhandler(change):\n",
    "    variable = change.new\n",
    "    update_plot_test(variable)\n",
    "\n",
    "variable_dropdown.observe(dropdown_eventhandler, 'value')\n",
    "\n",
    "# Display the dropdown and the plot\n",
    "display(widgets.VBox([variable_dropdown, plot_output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e47064",
   "metadata": {},
   "source": [
    "### Apply Model to Ungrouped SEA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sea_ungrouped = pd.read_feather(\"/capstone/mosaiks/repos/modeling/data/model_directory/SEA_ungroup_features_simple_impute_mean.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sea = features_sea_ungrouped.iloc[:, 2:12002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3219dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dictionaries for storing the predicted values and R2 scores\n",
    "y_pred_sea = pd.DataFrame()\n",
    "\n",
    "# Iterate over the keys in models dictionary\n",
    "for target_column in models.keys():\n",
    "    # Get the corresponding trained model for the target column\n",
    "    model = models[target_column]\n",
    "    \n",
    "    # Make predictions for the target column\n",
    "    y_pred_sea_column = np.maximum(model.predict(features_sea), 0)\n",
    "    \n",
    "    # Store the predicted values and R2 score in their respective dictionaries\n",
    "    y_pred_sea[target_column] = y_pred_sea_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns from features\n",
    "selected_columns_sea = features_ungrouped[['lat', 'lon', 'year']]\n",
    "\n",
    "# Concatenate selected_columns with y_preds\n",
    "sea_preds = pd.concat([selected_columns_sea, y_pred_sea], axis=1)\n",
    "\n",
    "# Display the combined dataframe\n",
    "sea_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_preds.to_feather(\"/capstone/mosaiks/repos/modeling/data/predictions/SEA_predictions_ungrouped.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530b165d",
   "metadata": {},
   "source": [
    "## Apply Model to Zambia 10% Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71096b5-bd08-42c2-854d-655f56b6b2a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zambia = pd.read_feather(\"/capstone/mosaiks/repos/modeling/data/model_directory/zambia_10percent_features_simple_impute_modelpredict.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zambia_features = zambia.iloc[:,2:12002]\n",
    "zambia_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591764c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dictionaries for storing the predicted values and R2 scores\n",
    "y_pred_zambia = pd.DataFrame()\n",
    "\n",
    "# Iterate over the keys in models dictionary\n",
    "for target_column in models.keys():\n",
    "    # Get the corresponding trained model for the target column\n",
    "    model = models[target_column]\n",
    "    \n",
    "    # Make predictions for the target column\n",
    "    y_pred_zambia_column = np.maximum(model.predict(zambia_features), 0)\n",
    "    \n",
    "    # Store the predicted values and R2 score in their respective dictionaries\n",
    "    y_pred_zambia[target_column] = y_pred_zambia_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03c71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_zambia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns from features\n",
    "selected_columns_zambia = zambia[['lat', 'lon', 'year']]\n",
    "\n",
    "# Concatenate selected_columns with y_preds\n",
    "zambia_preds = pd.concat([selected_columns_zambia, y_pred_zambia], axis=1)\n",
    "\n",
    "# Display the combined dataframe\n",
    "zambia_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb04cdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zambia_preds.to_feather(\"/capstone/mosaiks/repos/modeling/data/predictions/zambia_10perc_predictions.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_preds = pd.read_csv('capstone/mosaiks/repos/modeling/data/predictions/SEA_predictions_ungrouped.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38476b4e-d3b5-4f79-8d2d-651d6416900a",
   "metadata": {},
   "source": [
    "### Congratulations on completing this analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mosaiks-modeling",
   "language": "python",
   "name": "mosaiks-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
