{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52910d0-39ee-45ee-8eee-d36bcfc47455",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d9028-88fb-4030-a31f-820cceb83995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1955461-95fe-4a4a-a40e-eea75a0923e8",
   "metadata": {},
   "source": [
    "This notebook will be used to prepare our feature and ground-truth data for our modeling process. At this point, we have a directory of .feather files that contain our features (see this notebook for feature generation). We want to concatentate these together, and perform the necessary operations to achieve a dataframe where a row is one observation unit; its columns are the associated features and the ground truth data that we want to train the model on. Broadly, we want our dataframe to take on the following form:\n",
    "\n",
    "| observation_unit |  outcome_1 | outcome_2 | feature_1| feature_2 | feature_3\n",
    "| ----|  ---- | ---- | -- | -- | -- |\n",
    "| 1   |  y_1 | x_1 | x_2 | x_3 | x_4\n",
    "| 2   |  y_2 | x_1 | x_2| x_3 | x_4\n",
    "\n",
    "Where we can run regress `outcome_1` and `outcome_2` on `feature_1`, `feature_2` and `feature_3`\n",
    "\n",
    "In our case, an observation is one SEA/year. We have ground truth data that summarizes the total crop yield, total crop loss, as well as other pertinent agricultural variables. We want our dataframe described above to look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d4949-8246-4938-b2e5-c012918decf3",
   "metadata": {},
   "source": [
    "| SEA | Year | Observed Yield (Tonnes) | Observed Loss (Tonnes) | feature1| feature2 | feature3\n",
    "| ----| ----| ---- | ---- | -- | -- | -- |\n",
    "| 1   | 2016 | 72 | 13 | 1.23 | 3.25 | 0.123\n",
    "| 2   | 2016  | 50 | 7.5 | 0.78| 1.2 | 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf02eb13-a9aa-4fb6-90d7-dd73cf572ac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/capstone/mosaiks/repos/modeling/data/landsat-8_cropmosaiks_features/landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_2013.feather\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/capstone/mosaiks/repos/modeling/data/landsat-8_cropmosaiks_features/landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_2014.feather\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/capstone/mosaiks/repos/modeling/data/landsat-8_cropmosaiks_features/landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_2017.feather\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# read in each file as a dataframe and concatenate them together\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mconcat([pd\u001b[38;5;241m.\u001b[39mread_feather(file_path) \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# print the resulting dataframe\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# define the file paths for each file\n",
    "feature_dir = \"/capstone/mosaiks/repos/modeling/data/landsat-8_cropmosaiks_features/\" #insert path to feature .feather files here\n",
    "file_paths = [\n",
    "    \"/capstone/mosaiks/repos/modeling/data/landsat-8_cropmosaiks_features/landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_2013.feather\",\n",
    "    \"/capstone/mosaiks/repos/modeling/data/landsat-8_cropmosaiks_features/landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_2014.feather\",\n",
    "    \"/capstone/mosaiks/repos/modeling/data/landsat-8_cropmosaiks_features/landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_2015.feather\",\n",
    "    \"/capstone/mosaiks/repos/modeling/data/landsat-8_cropmosaiks_features/landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_2016.feather\",\n",
    "    \"/capstone/mosaiks/repos/modeling/data/landsat-8_cropmosaiks_features/landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_2017.feather\",\n",
    "]\n",
    "\n",
    "# read in each file as a dataframe and concatenate them together\n",
    "df = pd.concat([pd.read_feather(file_path) for file_path in file_paths])\n",
    "\n",
    "# print the resulting dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51a99d-689f-4b84-ad90-e8d031807d2e",
   "metadata": {},
   "source": [
    "### Read in Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584827fc-c6b2-4b72-ba73-29c0c3de804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the new concatenated features:\n",
    "features = pd.read_feather(\"/capstone/mosaiks/repos/modeling/data/cropmosaiks_features_landsat8.feather\")\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5c4de1-3a31-40df-9551-b24e80fe82bc",
   "metadata": {},
   "source": [
    "### Read in Ground-Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56a91a-1e07-4cfd-9562-6d2d286cf9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the survey data\n",
    "country_sea = gpd.read_file('/capstone/mosaiks/repos/preprocessing/featurizeme/total.shp')\n",
    "country_sea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f6378-df8f-4eb1-ba56-887cface02ef",
   "metadata": {},
   "source": [
    "### Spatially join features to Ground-Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452991a1-fdf1-49d1-82c8-c349fde3865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter country_sea for unique values of 'seq_unq' and 'geometry'\n",
    "sea_unq_join = country_sea[['sea_unq', 'geometry']].drop_duplicates()\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(sea_unq_join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a290900-ca71-4a41-83be-6cd6fb351559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize the features by growing season\n",
    "# Carry months October, November, and December over to the following year's data\n",
    "# These months represent the start of the growing season for the following year's maize yield\n",
    "year_end = 2022\n",
    "\n",
    "features['year'] = np.where(\n",
    "    features['month'].isin([10, 11, 12]),\n",
    "    features['year'] + 1, \n",
    "    features['year'])\n",
    "\n",
    "features_gs = features[features['year'] <= year_end]\n",
    "\n",
    "features_gs.sort_values(['year', 'month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62de2c30-e51d-4dab-b4b9-ac798becab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a geodataframe of the new features\n",
    "features_new_gdf = gpd.GeoDataFrame(\n",
    "    features_gs, \n",
    "    geometry = gpd.points_from_xy(x = features_gs.lon, y = features_gs.lat), \n",
    "    crs='EPSG:4326'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812a336-d912-42d5-9c97-0ae0f87e574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the 'geometry' column separately before unstacking\n",
    "geometry_col = features_new_gdf[['lon', 'lat', 'geometry']].drop_duplicates(subset=['lon', 'lat'])\n",
    "\n",
    "# Perform the unstacking operation without the 'geometry' column\n",
    "features_gs_no_geometry = features_gs.drop(columns=['geometry'])\n",
    "features = features_gs_no_geometry.set_index(['lon', 'lat', 'year', 'month']).unstack()\n",
    "features.columns = features.columns.map(lambda x: '{}_{}'.format(*x))\n",
    "\n",
    "# Merge the 'geometry' column back into the features DataFrame\n",
    "features = features.reset_index().merge(geometry_col, on=['lon', 'lat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc382c3d-dcc5-4fe8-b408-d492330ce8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unwanted 'index' and 'geometry' columns\n",
    "features = features.filter(regex='^(?!index_)')\n",
    "\n",
    "\n",
    "# Convert the 'features' DataFrame to a GeoDataFrame\n",
    "features_gdf = gpd.GeoDataFrame(features, geometry=features['geometry'], crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbce82e-0343-4d46-bf75-ef1f48e99dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets combine the sea data \n",
    "spatial_join = gpd.sjoin(features_gdf, sea_unq_join, how='right', predicate = 'within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020e4de-fb90-4184-a8d2-590583ded72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_join = spatial_join.merge(country_sea, on=['year', 'sea_unq'], how='inner')\n",
    "# Drop the redundant independent lon and lat columns because now that they are in a separate geometry column\n",
    "features_join = features_join.drop(['lon', 'lat', 'geometry_x'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3382ce-0a50-45e8-9d65-0ad0f9aa45d8",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "\n",
    "### Impute missing values\n",
    "\n",
    "Imputing \"manually\" by descending group levels imputes NA values in multiple \"cascading\" steps, decreasing the proportion of inputed values with each step. First, the NA values are imputed at by both `year` and `geometry`, which should yield imputed values that most closely match the feature values that would be present in the data if there was no clouds obscuring the satellite images. Next, the remaining NA values that could not be imputed by both `year` and `district` are imputed by only `district`. Lastly, the remaining NA vlaues that could not be imputed by both `year` and `district` or by just `district` are imputed by `year` only. This option gives the user more control and transparency over how the imputation is executed.\n",
    "\n",
    "Imputing using `scikit learn`'s simple imputer executes standard imputation, the details of which can be found in the `scikitlearn` documentation [here.](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "\n",
    "The imputation approach depends on the selection made at the top of this notebook for `impute_manual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6c93f-4046-4d49-a4f2-64e1d1b7bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the size of the features dataframe\n",
    "rows, cols = features_join.shape\n",
    "\n",
    "# compute the number of feature cells in the features dataframe\n",
    "num_cells = rows * cols\n",
    "num_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ad2ef-4c20-4fd3-be2b-0a30139ad323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    BL = '\\x1b[1;34m' #GREEN\n",
    "    GR = '\\x1b[1;36m' #GREEN\n",
    "    YL = '\\x1b[1;33m' #YELLOW\n",
    "    RD = '\\x1b[1;31m' #RED\n",
    "    RESET = '\\033[0m' #RESET COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b830b-b240-48f3-b9f5-79f77e12c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes: Have to change the year, get an error rn. Also, check to make sure the number of cells is correct\n",
    "if impute_manual:\n",
    "    ln_ft = len(features_join)\n",
    "    ln_na = len(features_join.dropna())\n",
    "    print(f'Starting total row count: {bcolors.BL}{ln_ft}{bcolors.RESET}',\n",
    "          f'\\nPre-Impute NaN row count: {bcolors.RD}{ln_ft - ln_na}{bcolors.RESET}',\n",
    "          f'\\nPre-Impute NaN row %: {bcolors.RD}{((ln_ft - ln_na) / ln_ft)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\nPre-Impute NaN cell %: {bcolors.RD}{(features_join.isna().sum().sum() / num_cells)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\n\\nStep 1: Filling NaN values by month, year, and district group average')\n",
    "    features_join = (\n",
    "        features_join\n",
    "        .fillna(features_join\n",
    "                .groupby(['year', 'sea_unq'], as_index=False) \n",
    "                .transform('mean')\n",
    "               )\n",
    "    )\n",
    "    ln_ft = len(features_join)\n",
    "    ln_na = len(features_join.dropna())\n",
    "    print(f'Post step 1 NaN row count: {bcolors.YL}{ln_ft - ln_na}{bcolors.RESET}',\n",
    "          f'\\nPost step 1 NaN row %: {bcolors.YL}{((ln_ft - ln_na) / ln_ft)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\nPost step 1 NaN cell %: {bcolors.YL}{(features_join.isna().sum().sum() / num_cells)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\n\\nStep 2: Filling NaN values by month and district across group average')\n",
    "    features_join = (\n",
    "        features_join\n",
    "        .fillna(features_join\n",
    "                .groupby(['sea_unq'], as_index=False)\n",
    "                .transform('mean')\n",
    "               )\n",
    "    )\n",
    "    ln_ft = len(features_join)\n",
    "    ln_na = len(features_join.dropna())\n",
    "    print(f'Post step 2 NaN row count: {bcolors.GR}{ln_ft - ln_na}{bcolors.RESET}',\n",
    "          f'\\nPost step 2 NaN row %: {bcolors.GR}{((ln_ft - ln_na) / ln_ft)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\nPost step 2 NaN cell %: {bcolors.GR}{(features_join.isna().sum().sum() / num_cells)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\n\\nStep 3: Drop remaining NaN values\\n')\n",
    "    features_join = features_join.dropna(axis=0)\n",
    "    print(f'Ending total row count: {bcolors.BL}{len(features_join)}{bcolors.RESET}')\n",
    "else:\n",
    "    # Store the geometry column separately\n",
    "    geometry_col = features_join['geometry_y']\n",
    "    # Remove the geometry column from the DataFrame\n",
    "    features_join = features_join.drop(columns=['geometry_y'])\n",
    "    features_join = features_join.set_index(['year', 'sea_unq'])\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputer.fit_transform(features_join)\n",
    "    features_join[:] = imputer.transform(features_join)\n",
    "    features_join = features_join.reset_index()\n",
    "    # Add the geometry column back to the DataFrame\n",
    "    features_join['geometry'] = geometry_col"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mosaiks-modeling",
   "language": "python",
   "name": "mosaiks-modeling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
