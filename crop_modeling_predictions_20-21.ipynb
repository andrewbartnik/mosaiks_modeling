{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe56655",
   "metadata": {},
   "source": [
    "# Modeling Crop Yield\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import geopandas\n",
    "\n",
    "import pyarrow\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.linalg import LinAlgWarning\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2f0e8f",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "#### Choose a satellite.\n",
    "\n",
    "For a description of the Landsat 8 mission, see the US Geological metadata [here.]()\n",
    "\n",
    "For a description of the Sentinel 2 mission, see the US Geological metadata [here.]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb13001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# satellite = \"landsat-8-c2-l2\"\n",
    "satellite = \"sentinel-2-l2a\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ab70e",
   "metadata": {},
   "source": [
    "#### Choose band combination.\n",
    "\n",
    "For a description of **Landsat** bands, see the [US Geological Survey documentation here.](https://www.usgs.gov/faqs/what-are-band-designations-landsat-satellites)\n",
    "\n",
    "For a description of **Sentinel bands**, see the [US Geological Survey documentation here.](https://www.usgs.gov/centers/eros/science/usgs-eros-archive-sentinel-2#:~:text=4%20bands%20at%2010%20meter,%2Dinfrared%20(842%20nm)\n",
    "\n",
    "According to our results, bands **(insert band selection here)** result in the best model performance for Landsat, and **(insert band selection here)** result in the best model performance for Sentinel for the task of predicting maize yields in Zambia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bands = \"2-3-4\"\n",
    "bands = \"2-3-4-8\"\n",
    "# bands = \"1-2-3-4-5-6-7\"\n",
    "# bands = \"2-3-4-5-6-7-8-11-12\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ecb9d",
   "metadata": {},
   "source": [
    "#### Choose the number of points that were featurized.\n",
    "\n",
    "Each value in the following chunk represents the amount of thousands of points that were featurized in each respective feature file. These points represent a uniform subset of the spatial grid of Zambia. Points are spaced at uniform intervals for each selection, measured in kilometers in the longitudinal direction for each set of features. The kilometer distance interval differs for each selection below; 42,000 points results in the smallest uniform distance between points, and 4,000 points results in the greatest uniform distance between points. Selecting a greater quantity of points results in a denser spatial sample, which increases computational cost and time, but increases the spatial resolution of the model. Regardless of the quantity of points selected, each point is buffered by the same distance, resulting in a 1km^2 cell around each point.\n",
    "\n",
    "These specific options point quantities is a result of uniformly increasing the distance between points in units of kilometers prior to matching satellite images to each point. These options represent the number of points that fall within the borders of Zambia, and the numbers have been rounded to the nearest thousandth for consistency in naming files. See the [CropMOSAIKS Featurization repository](https://github.com/cropmosaiks/Featurization) for more information regarding how these distances we calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a903025",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = 15\n",
    "# points = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b135301",
   "metadata": {},
   "source": [
    "#### Choose to keep only areas with crops (`True`) or to keep all points (`False`)\n",
    "\n",
    "Selecting `True` applies a \"cropland mask\" to the spatial grid of Zambia. This retains only the regions of the country in which maize is  grown, according to the **(insert source here)**. As a result, the spatial extent of the features that are fed into the model are highly subset for the specific task at hand: modeling maize yields. According to our results, selecting `True` **(insert increases or decreases here)** model performance.\n",
    "\n",
    "Selecting `False` results in modeling with the maximum spatial extent of the features, with more generalized features as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_mask = True\n",
    "#crop_mask = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede5fbb",
   "metadata": {},
   "source": [
    "Choose a weighted average (`True`) or a simple mean (`False`) to use when collapsing features to administrative boundary level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccb6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weighted_avg = True\n",
    "weighted_avg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca0aad3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Choose which months to use in the model.\n",
    "\n",
    "Note that months 10, 11, and 12 get pushed to the next year because the growing season (November - May) spans the calendar year. Maize is planted in November, starts to change color with maturity in May, and is harvested in June - August. According to our results, subsetting the months to **(insert month selection here)** increases model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "# month_range = [      3, 4, 5, 6, 7, 8, 9            ]\n",
    "# month_range = [         4, 5, 6, 7, 8, 9            ]\n",
    "# month_range = [            5, 6, 7, 8, 9            ]\n",
    "# month_range = [         4, 5, 6, 7, 8               ]\n",
    "# month_range = [            5, 6, 7, 8               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c15b7c5",
   "metadata": {},
   "source": [
    "#### Impute NA values by descending group levels (True) or `scikit learn`'s simple imputer (False)\n",
    "\n",
    "Imputing \"manually\" by descending group levels imputes NA values in multiple \"cascading\" steps, decreasing the proportion of inoutated values with each step. First, the NA values are imputed at by both `year` and `district`, which should yield imputed values that most closely match the feature values that would be present in the data if there was no clouds obscuring the satellite images. Next, the remaining NA values that could not be imputed by both `year` and `district` are imputed by only `district`. Lastly, the remaining NA vlaues that could not be imputed by both `year` and `district` or by just `district` are imputed by `year` only. This option gives the user more control and transparency over how the imputation is executed.\n",
    "\n",
    "Imputing using `scikit learn`'s simple imputer executes standard imputation, the details of which can be found in the `scikitlearn` documentation [here.](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9662e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute_manual = True\n",
    "impute_manual = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06291c4",
   "metadata": {},
   "source": [
    "### Unchanging parameters\n",
    "\n",
    "The parameters in the following chunk are set for the country of Zambia for with 1000 features, regardless of the satellite selected. The start years for each satellite reflect the respective years that Landsat 8 and Sentinel 2A missions began.\n",
    "\n",
    "The number of features is set to 1000 to serve as a staple parameter among the several other parameters varied during the model optimization process. Changing this parameter in the following code chunk will result in an error because featurizing landsat imagery for a different number of features was outside the scope of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f29eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code = \"ZMB\"\n",
    "num_features = 1000\n",
    "\n",
    "if satellite == \"landsat-8-c2-l2\":\n",
    "    year_start = 2013 # Landsat\n",
    "else:\n",
    "    year_start = 2015 # Sentinel\n",
    "year_end = 2021\n",
    "\n",
    "taylor_data_dir = \"/capstone/cropmosaiks/data\"  \n",
    "feature_file_name = (f'{satellite}_bands-{bands}_{country_code}_{points}k-points_{num_features}-features')\n",
    "weight_file_name = (f'{country_code}_crop_weights_{points}k-points')\n",
    "\n",
    "if points == \"4\":\n",
    "    marker_sz = 60\n",
    "elif points == \"15\":\n",
    "    marker_sz = 15\n",
    "elif points == \"24\":\n",
    "    marker_sz = 10\n",
    "else:\n",
    "    marker_sz = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6bb69",
   "metadata": {},
   "source": [
    "## Administrative boundaries \n",
    "\n",
    "Administrative boundaries reflect the **(insert number of districts in dataset)** district boundaries within the country of Zambia. A district can be likened to a state within the larger U.S.A. We subset the spatial grid to district level becuase the crop yield data is at the district level of specificity. The features are originally produced at higher spatial resolution, then summarized to the district level in order to train the model with ground-truth crop data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28fd026",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_shp = geopandas.read_file(f'{taylor_data_dir}/boundaries/gadm36_{country_code}_2.shp')\n",
    "country_shp = country_shp.rename(columns = {'NAME_2': 'district'})[['district', 'geometry']]\n",
    "country_shp.district = country_shp.district.replace(\"MPongwe\", 'Mpongwe', regex=True)\n",
    "country_districts = country_shp.district.sort_values().unique().tolist()\n",
    "country_shp = country_shp.set_index('district')\n",
    "country_shp.shape\n",
    "country_shp.plot(linewidth = 1, edgecolor = 'black' )\n",
    "# country_shp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ba4ec",
   "metadata": {},
   "source": [
    "## Crop yield\n",
    "\n",
    "Zambian maize yield data reflects the predicted annual maize yield provided by farmers in the month of May, when the maize matures and changes colors prior to harvest, which allows the farmers to estimate what their yield will be in the following months. These predictions are in units of metric tons per hectare and provide valuable insight to the Zambian government as they plan for the quanitites of food to import into the country in the future. For more metadata, see the websites for the [Central Statistics Office of Zambia (CSO)](https://www.zamstats.gov.zm/) and the [Summary statistics from CSO.](https://www.zamstats.gov.zm/agriculture-environment-statistics/)\n",
    "\n",
    "In order to standardize the names of all districts shared between the geoboundaries and the crop yield data, we correct for spelling, dashes, and apostrophes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_df = pd.read_csv(taylor_data_dir+'/crops/cfs_maize_districts_zambia_2009_2018.csv')\n",
    "crop_df.district = crop_df.district.replace(\n",
    "    {\"Itezhi-tezhi\": 'Itezhi-Tezhi',\n",
    "     \"Kapiri-Mposhi\": 'Kapiri Mposhi',\n",
    "     \"Shang'ombo\": 'Shangombo',\n",
    "     \"Chienge\": 'Chiengi'\n",
    "    }, regex=True)\n",
    "crop_districts = crop_df.district.sort_values().unique().tolist()\n",
    "crop_df = crop_df[['district', 'year', 'yield_mt']]\n",
    "ln = len(crop_df[crop_df.year == 2016].district)\n",
    "crop_df = crop_df.set_index('district')\n",
    "ln\n",
    "# crop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(crop_districts) - set(country_districts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c55cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(country_districts) - set(crop_districts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdc3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_crop = geopandas.GeoDataFrame(crop_df.join(country_shp), crs = country_shp.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace3e2f",
   "metadata": {},
   "source": [
    "## Crop land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.read_feather(f\"{taylor_data_dir}/weights/{weight_file_name}.feather\")\n",
    "# weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_gdf = geopandas.GeoDataFrame(\n",
    "    weights, \n",
    "    geometry = geopandas.points_from_xy(x = weights.lon, y = weights.lat), \n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "weights_gdf.plot(figsize = (10,10),\n",
    "                 cmap = 'inferno',\n",
    "                 markersize = marker_sz,\n",
    "                 alpha = .9,\n",
    "                 column = 'crop_perc')\n",
    "# plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa0cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.crop_perc = weights.crop_perc.fillna(0)\n",
    "# #weights.crop_perc = weights.crop_perc + 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa5a1f",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Append annual features files together into one file: `features_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55409ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw = geopandas.GeoDataFrame()\n",
    "\n",
    "for yr in range(year_start, year_end + 1):\n",
    "    print(f\"Opening: {feature_file_name}_{yr}.feather\")\n",
    "    features_x = pd.read_feather(f\"{taylor_data_dir}/features/{satellite}/{feature_file_name}_{yr}.feather\")\n",
    "    \n",
    "    if (yr == 2013) & (satellite == \"landsat-8-c2-l2\"):\n",
    "        features_x = features_x[features_x.month > 9]\n",
    "    elif (yr == 2015) & (satellite == \"sentinel-2-l2a\"):\n",
    "        features_x = features_x[features_x.month > 9]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # concatenate the feather files together, axis = 0 specifies to stack rows (rather than adding columns)\n",
    "    features_raw = pd.concat([features_raw, features_x], axis=0)\n",
    "    \n",
    "    print(\"feature.shape\", features_raw.shape)\n",
    "    print(\"Appending:\", yr)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of the raw features before we convert growing season months at the end of the year to the next year, summarise to district level, and other processing so we can plot raw features later at feature-level resolution\n",
    "features = features_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry months October, November, and December over to the following year's data\n",
    "# these months represent the start of the growing season for the following year's maize yield\n",
    "features['year'] = np.where(\n",
    "    features['month'].isin([10, 11, 12]),\n",
    "    features['year'] + 1, \n",
    "    features['year'])\n",
    "\n",
    "features = features[features['year'] <= year_end]\n",
    "\n",
    "features.sort_values(['year', 'month'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8539a4a",
   "metadata": {},
   "source": [
    "### Filter month range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b2d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the features to only the month range selected at the top of the notebook\n",
    "features = features[features.month.isin(month_range)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554ff055",
   "metadata": {},
   "source": [
    "### Pivot wider\n",
    "Here we pivot the data from long format to wide by indexing on 'lon', 'lat', 'year', 'month' and using the unstack function. We then map column names based on the month index and the associated features so month '01' is appended to each feature for that month making 0_01, 1_01 etc. This results in a Tidy data structure, with each row representing an image, and each column representing a feature for a certain month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.set_index(['lon','lat', \"year\", 'month']).unstack()\n",
    "features.columns = features.columns.map(lambda x: '{}_{}'.format(*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e9b16e",
   "metadata": {},
   "source": [
    "### Replace \"inf\" values with `NaN`\n",
    "\n",
    "Infinity values are the result of **(insert reason here)**. We replace them with `NaN` because **(insert reason here)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845e697",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "features = features.reset_index()\n",
    "# features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a234a9c",
   "metadata": {},
   "source": [
    "### Attach crop weights\n",
    "Attach weight to each point (% area cropped of surrounding 1 km^2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.join(weights.set_index(['lon', 'lat']), on = ['lon', 'lat'])\n",
    "features = features.drop([\"geometry\"], axis = 1)\n",
    "# features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3147c",
   "metadata": {},
   "source": [
    "### Mask croppped regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d65ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any 1 km^2 cell with a crop percentage > 0 will be retained\n",
    "# the mask will not be applied if crop_mask is set to False at the top of this notebook\n",
    "if crop_mask:\n",
    "    features = features[features.crop_perc > 0]\n",
    "else:\n",
    "    pass\n",
    "# features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3acd3",
   "metadata": {},
   "source": [
    "### Make \"features\" a `GeoDataFrame`\n",
    "\n",
    "The coordinate reference system is set to EPSG 4326 - WGS 84, the latitude/longitude coordinate system based on the Earth's center of mass, used by the Global Positioning System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee1f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = geopandas.GeoDataFrame(\n",
    "    features, \n",
    "    geometry = geopandas.points_from_xy(x = features.lon, y = features.lat), \n",
    "    crs='EPSG:4326'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d6ff1",
   "metadata": {},
   "source": [
    "### Plot any single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f447f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mn = 9\n",
    "# yr = 2017\n",
    "# feature = 999\n",
    "\n",
    "# features[features.year == yr].plot(\n",
    "#     column = f\"{feature}_{mn}\",\n",
    "#     figsize = (10,10),\n",
    "#     marker='H',\n",
    "#     # legend = True,\n",
    "#     markersize = marker_sz,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc66e85d",
   "metadata": {},
   "source": [
    "### Drop 'lat' and 'lon' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b47a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the redundant independent lon and lat columns because now that they are in a separate geometry column, there is no need for these columns\n",
    "features = features.drop(['lon', 'lat'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4c25e0",
   "metadata": {},
   "source": [
    "### Join features to country geometry\n",
    "\n",
    "Join the partially processed feature data to Zambia geometry to join to district level (the highest resolution at which we have maize yield data). After imputation and some minor data porcessing steps, the features will be summarized to each district level. Although this lowers spatial resolution of the features, it is a necessary step to train the model because the district-level crop yields need to be paired with the district-level features in order to execute supervised machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fba002",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.sjoin(country_shp, how = 'left', predicate = 'within')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c217e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# na = features[adm_features.isna().any(axis = 1)]\n",
    "# na.plot(figsize = (10,10), markersize = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3230ad6",
   "metadata": {},
   "source": [
    "### Correct column names and drop geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84705c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA's from the district column (called index_right) then rename the column index_right to district\n",
    "features = (\n",
    "    features\n",
    "    # drop NA values\n",
    "    .dropna(subset=['index_right'])\n",
    "    .rename(columns = {\"index_right\": \"district\",})\n",
    "    .reset_index(drop = True)\n",
    ")\n",
    "# make a copy of the features, assigned to an object so we can plot the points as they are in this state\n",
    "# later, after imputation, we also copy the entire features df and save that as an object in case we want to plot or work with them in that stage \n",
    "points = features.copy()\n",
    "# save the geometries as an object to join them later to the rows of crop yield predictions\n",
    "points = features[['geometry']]\n",
    "# save the years as an object to join them later to the rows\n",
    "year = features[['year']]\n",
    "# drop geometry column for 20/21 features\n",
    "features = features.drop(['geometry'], axis = 1)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deeefc9-f501-4843-a5e3-66d02988aeea",
   "metadata": {},
   "source": [
    "### Intermediate Processing Check: Plot the Features\n",
    "\n",
    "So far, the features have been masked to just the crop regions of Zambia. Therefore, we expect the plotted points to roughly show the border of Zambia and be present throughout the country, with certain chunks of points missing in areas where crops are not grown. This serves as a \"sanity check\" that we are on-track with our spatial processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5753fa30-32e2-44ce-972e-e33d5d567f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "points.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca644884",
   "metadata": {},
   "source": [
    "### Impute missing values\n",
    "\n",
    "Imputing \"manually\" by descending group levels imputes NA values in multiple \"cascading\" steps, decreasing the proportion of inoutated values with each step. First, the NA values are imputed at by both `year` and `district`, which should yield imputed values that most closely match the feature values that would be present in the data if there was no clouds obscuring the satellite images. Next, the remaining NA values that could not be imputed by both `year` and `district` are imputed by only `district`. Lastly, the remaining NA vlaues that could not be imputed by both `year` and `district` or by just `district` are imputed by `year` only. This option gives the user more control and transparency over how the imputation is executed.\n",
    "\n",
    "Imputing using `scikit learn`'s simple imputer executes standard imputation, the details of which can be found in the `scikitlearn` documentation [here.](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)\n",
    "\n",
    "The imputation approach depends on the selection made at the top of this notebook for `impute_manual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9cfbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute the number of cells in the features dataframe, based on the amount of rows (images), months, and feature columns\n",
    "num_cells = len(features) * len(month_range) * num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a25df-4114-491d-9609-f88ea8120405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200efa01-e2eb-460c-8f29-d0f525d1e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfeatures = dd.from_pandas(features, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7c28d-e99b-4a65-830e-21dd41909f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if impute_manual:\n",
    "#     with Client(n_workers=16) as client:\n",
    "        \n",
    "#         print(f'Total rows: {(len(dfeatures))} \\nPre-Impute NaN row count: {(len(dfeatures) - len(dfeatures.dropna()))} ',\n",
    "#               f'\\nPre-Impute NaN row %: {((len(dfeatures) - len(dfeatures.dropna())) / len(dfeatures))*100:.02f}',\n",
    "#               f'\\nPre-Impute NaN cell %: {(dfeatures.isna().sum().sum() / num_cells)*100:.02f}')\n",
    "\n",
    "#         print(f'\\nStep 1: Filling NaN values by month, year, and district group average')\n",
    "#         dfeatures = (\n",
    "#             dfeatures\n",
    "#             .fillna(dfeatures\n",
    "#                     .groupby(['year', 'district'], as_index=False)\n",
    "#                     .transform('mean')\n",
    "#                    )\n",
    "#                 #.compute()\n",
    "#         )\n",
    "#         print(f'{(dfeatures.isna().sum().sum() / num_cells)*100:.02f} % NaN cell values after imputing step 1')\n",
    "\n",
    "#         print(f'\\nStep 2: Filling NaN values by month and district across group average')\n",
    "#         dfeatures = (\n",
    "#             dfeatures\n",
    "#             .fillna(dfeatures\n",
    "#                     .groupby(['district'], as_index=False)\n",
    "#                     .transform('mean')\n",
    "#                    )\n",
    "# #                 .compute()\n",
    "#         )\n",
    "#         print(f'{(dfeatures.isna().sum().sum() / num_cells)*100:.02f} % NaN cell values after imputing step 2')\n",
    "\n",
    "#         print('\\nStep 3: Drop remaining NaN values')\n",
    "\n",
    "#         print(f'Total rows: {(len(dfeatures))} \\nPost-Impute NaN row count: ',\n",
    "#               f'{(len(dfeatures) - len(dfeatures.dropna()))} \\nPost-Impute NaN row %: ',\n",
    "#               f'{((len(dfeatures) - len(dfeatures.dropna())) / len(dfeatures))*100:.02f}\\n', sep = \"\")\n",
    "#         dfeatures = dfeatures.dropna(axis=1)\n",
    "#     #         .compute()\n",
    "#         print(f'{(dfeatures.isna().sum().sum() / num_cells)*100:.02f} % NaN cell values\\n')\n",
    "# else:\n",
    "#     dfeatures = dfeatures.set_index(['year', 'district'])\n",
    "#     imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "#     imputer.fit_transform(dfeatures)\n",
    "#     dfeatures[:] = imputer.transform(dfeatures)\n",
    "#     dfeatures = dfeatures.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca7eda-15bb-4d0f-8706-f8665424aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    BL = '\\x1b[1;34m' #GREEN\n",
    "    GR = '\\x1b[1;36m' #GREEN\n",
    "    YL = '\\x1b[1;33m' #YELLOW\n",
    "    RD = '\\x1b[1;31m' #RED\n",
    "    RESET = '\\033[0m' #RESET COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1be2c7c-37e1-4bf9-80fc-d0554a14f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if impute_manual:\n",
    "    ln_ft = len(features)\n",
    "    ln_na = len(features.dropna())\n",
    "    print(f'Starting total row count: {bcolors.BL}{ln_ft}{bcolors.RESET}',\n",
    "          f'\\nPre-Impute NaN row count: {bcolors.RD}{ln_ft - ln_na}{bcolors.RESET}',\n",
    "          f'\\nPre-Impute NaN row %: {bcolors.RD}{((ln_ft - ln_na) / ln_ft)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\nPre-Impute NaN cell %: {bcolors.RD}{(features.isna().sum().sum() / num_cells)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\n\\nStep 1: Filling NaN values by month, year, and district group average')\n",
    "    features = (\n",
    "        features\n",
    "        .fillna(features\n",
    "                .groupby(['year', 'district'], as_index=False)\n",
    "                .transform('mean')\n",
    "               )\n",
    "    )\n",
    "    ln_ft = len(features)\n",
    "    ln_na = len(features.dropna())\n",
    "    print(f'Post step 1 NaN row count: {bcolors.YL}{ln_ft - ln_na}{bcolors.RESET}',\n",
    "          f'\\nPost step 1 NaN row %: {bcolors.YL}{((ln_ft - ln_na) / ln_ft)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\nPost step 1 NaN cell %: {bcolors.YL}{(features.isna().sum().sum() / num_cells)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\n\\nStep 2: Filling NaN values by month and district across group average')\n",
    "    features = (\n",
    "        features\n",
    "        .fillna(features\n",
    "                .groupby(['district'], as_index=False)\n",
    "                .transform('mean')\n",
    "               )\n",
    "    )\n",
    "    ln_ft = len(features)\n",
    "    ln_na = len(features.dropna())\n",
    "    print(f'Post step 2 NaN row count: {bcolors.GR}{ln_ft - ln_na}{bcolors.RESET}',\n",
    "          f'\\nPost step 2 NaN row %: {bcolors.GR}{((ln_ft - ln_na) / ln_ft)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\nPost step 2 NaN cell %: {bcolors.GR}{(features.isna().sum().sum() / num_cells)*100:.02f}{bcolors.RESET}',\n",
    "          f'\\n\\nStep 3: Drop remaining NaN values\\n')\n",
    "    features = features.dropna(axis=0)\n",
    "    print(f'Ending total row count: {bcolors.BL}{len(features)}{bcolors.RESET}')\n",
    "else:\n",
    "    features = features.set_index(['year', 'district'])\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputer.fit_transform(features)\n",
    "    features[:] = imputer.transform(features)\n",
    "    features = features.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51dd396",
   "metadata": {},
   "source": [
    "### Save copy of completed data\n",
    "\n",
    "Duplicate the features dataframe so we can summarize features to district resolution for one set and still retain features at point resolution in other dataframe. Now that the `NA` values have been imputed or dropped, we might choose to plot these features for all years together _and_ we will feed this dataframe into the model (after it has been trained) in order to generate predictions for all years and demonstrate the overall performance. In this copy etap, ee preserve all features as they have undergone all processing besides the processing that will only be executed on the 2014-2018 features in order to prepare them for model training. The copy is called `features_copy` and the features that will be split and processed futher is called `features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d421fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_copy = features.copy()\n",
    "\n",
    "# assign the geometry col to features_copy so it can serve 2 purposes: 1. plotting and 2. the entire df can be fed into the model after it is trained on only the features for 2014-2018 and the associated crop data\n",
    "features_copy['geometry'] = points.geometry\n",
    "# moving forward, use `features` to summarize to district level (we dont need geoometries for that since `district` is already present in this df as a column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf565a2-5486-43a8-9eec-9a75ba19d21a",
   "metadata": {},
   "source": [
    "### Split features into `2014-2018` and `2019-2021`\n",
    "\n",
    "All feature data required the processing steps executed above. However, the following code is not executed on _all_ features.\\\n",
    "1. The `2014-2018` (Landsat 8) or `2015-2018` (Sentinel 2) set of features will be summarized to administrative boundary level (all rows (points) have already been assigned to the district in which they reside).\n",
    "2. These summarized features for `2014-2018` (Landsat 8) or `2015-2018` (Sentinel 2) will then be joined with the district-resolution crop data. Next, we split these features into train and test sets, and then use them to train the model using ridge regression.\n",
    "3. We will then apply the trained model to the large, comprehensive feature set for `2014-2021` (Landsat) or `2015-2021` (Sentinel), which includes all years regardless if those years have ground truth crop data. \n",
    "\n",
    "We are most interested in the crop predictions for the three years 2019-2021 because the model was not trained on these years. Therefore, we split the data here to execute different analysis moving forward. The feature data for `2019-2021` is called `features_without_crop_data` to be explicit about its context. The following code splits the data correctly no matter which satellite was selected at the top of the notebook.\n",
    "\n",
    "**Note:** After we train the model, if we are interested in running the model on ALL feature years `2014-2021`, rather than just the years without crop data, then we would feed in the features dataframe we copied in the section prior, called `features_copy`. If instead we are only interested in modeling for `2019-2021`, then we would later feed in the dataframe `features_without_crop_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023df42-a53c-4e92-8738-118ce91096eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data into 2 groups: 2014/2015-2018 for model training and 2014/2015-2021 for predicting on all years\n",
    "features_model_training = features[features.year <= 2018]\n",
    "\n",
    "# check that subsetting years worked\n",
    "features_model_training[\"year\"].unique()\n",
    "# should be 2016, 2017, 2018 for Sentinel\n",
    "# should be 2013, 2014, 2015, 2016, 2017, 2018 for Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885f72a3-b18d-4bb0-82b1-a18a1bc55b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_without_crop_data = features[features.year >= 2019]\n",
    "\n",
    "# check that subsetting years worked\n",
    "features_without_crop_data[\"year\"].unique()\n",
    "# should be 2019, 2020, 2021 regardless if the satellite is Sentinel or Landsat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7398ff",
   "metadata": {},
   "source": [
    "### Summarise to administrative boundary level\n",
    "Weighted by cropped area, or simple mean, depending on the selection at the top of this notebook for `weighted_avg`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check columns in the dataframe that will be fed into the ridge regression in order to train the model\n",
    "# we care about the order of columns specifically because in the following steps we assign only the feature columns to an object, so we need to know which 3 columns to omit by indexing\n",
    "features_model_training.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbc894-8004-4fa3-b02c-432e1acc84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape to the dataframe as a sanity check\n",
    "features_model_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310666d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object that contains only feature columns, rather than all columns that would include `district`, `year`, and `crop_perc`\n",
    "# python index starts at 0, so here we specify to retain columns starting at 3 through every column besides the last\n",
    "# the columns we omit stay in the dataframe, becuase we assign the selected columns to an object, but the omitted columns are not included in the calculation in the next chunk\n",
    "var_cols = features_model_training.columns[2:-1].values.tolist()\n",
    "# call the object `var_cols` to check that it only includes feature columsn, but do not view it in list format because it is more readable not as a list \n",
    "features_model_training.columns[2:-1]\n",
    "# these are all the feature columns that will be fed into the weighted_avg calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# finally, execute the weighted average on the cropped area! This gives more weight (importance) to area in Zambia that has a high proportion of cropland\n",
    "# this differs from the masking step that was executed earlier, because the masking step completely removed all land that was 0% cropland, and this step processes the land that is at least 1% cropland\n",
    "if weighted_avg:\n",
    "    features_summary = (\n",
    "        features_model_training\n",
    "        .groupby(['year', 'district'], as_index=False)\n",
    "        .apply(lambda x: pd.Series([sum(x[v] * x.crop_perc) / sum(x.crop_perc) for v in var_cols]))\n",
    "    )\n",
    "# if weighted_avg was set to FALSE at the beginning of the notebook, the weighted avergae is not executed\n",
    "# instead, the features are simply summarized by district and year, which also occurred above if weighted_avg was set to TRUE\n",
    "else:\n",
    "    features_summary = features_model_training.groupby(['district',\"year\"], as_index = False).mean()\n",
    "# now that we have renamed our features object to `features_summary`, note that we use this object moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5f1ce-57d8-46e4-bec3-c0bafe0d305e",
   "metadata": {},
   "source": [
    "Now that the features have been summarized to district and year, there are fewer rows. The dataframe we were working with before this step,  `features_model_training`, had 13866 rows that represented points. Now we have 216 rows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6e828-dd90-4ca8-b3f6-1c09096711e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_summary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1dc498",
   "metadata": {},
   "source": [
    "### Join crop data\n",
    "\n",
    "Combine the crop data and features into 1 dataframe in order to train the model on this object in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall the object `crop_df` from much earlier in the notebook \n",
    "crop_df_x = crop_df[crop_df.year >= year_start + 1]\n",
    "crop_df_x = crop_df_x[~crop_df_x.index.isin(['Mafinga', 'Ikelenge'])]\n",
    "crop_df_x.reset_index(inplace=True)\n",
    "# take a look at the crop dataframe as a sanity check that the processing steps were correct\n",
    "# the `yield_mt` data is in units of metric tonnes per hectare of cropland planted\n",
    "crop_df_x\n",
    "# if interested, please see more details about the crop data in our technical documentation for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_summary = (\n",
    "    features_summary\n",
    "    .set_index([\"district\", \"year\"])\n",
    "    .join(other = crop_df_x.set_index([\"district\", \"year\"]))\n",
    "    .reset_index())\n",
    "\n",
    "features_summary.columns\n",
    "# now that the data is joined, the column `yield_mt` is present in `features_summary`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3288b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new object that \n",
    "#model_year = features_summary[features_summary.year.isin([\n",
    "#   2014,\n",
    "#   2015,\n",
    "#   2016,\n",
    "#   2017,\n",
    "#   2018,\n",
    "#])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea11764",
   "metadata": {},
   "source": [
    "### Define `x's` and `y's` that will be a part of training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f3d06-6248-4c6b-bcc1-62a12b5ed0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this if-else statement is determined by the selection of True or False made at the top of the notebook for the object `weigthed_avg`\n",
    "if weighted_avg:\n",
    "    drop_cols = ['district', 'year', 'yield_mt']\n",
    "else:\n",
    "# if the selection was False, we can drop the `crop_perc` columns because we will not use it moving forward\n",
    "    drop_cols = ['district', 'year', 'yield_mt', \"crop_perc\"]\n",
    "# note that this chunk does not actually execute the column dropping, it just defines the object that will determine which are dropped in the next chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ALL x's (the features_summary data that will be fed into the model to train) by dropping the columns specified in the previous chunk\n",
    "# if the axis argument was set to 0, this code would incorrectly drop rows instead of columns\n",
    "x_all = features_summary.drop(drop_cols, axis = 1)\n",
    "\n",
    "# log transform the yield_mt column because that was recommended by the client\n",
    "y_all = np.log10(features_summary.yield_mt.to_numpy() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ccecfe-5e5c-4895-8fdb-d0d165364900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the first 10 values of the log transformed `yield_mt` array to get an idea of the scale of the crop yields we use to train the model\n",
    "print(y_all[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca312a5f",
   "metadata": {},
   "source": [
    "### Split into train and test sets\n",
    "\n",
    "This step is executed right before training the model so we can train on 80% of the data and preserve 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea5f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of total points: \", len(x_all), \"\\n\", \n",
    "      \"Number of training points: \", len(x_train), \"\\n\",\n",
    "      \"Number of testing points: \", len(x_test), sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2d84d0",
   "metadata": {},
   "source": [
    "### Train model using ridge regression\n",
    "\n",
    "Please see the documentation for the function that executes this regression [here.](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbadba3-9bef-43e2-b75f-8a5f9310f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf329e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = RepeatedKFold(n_splits = 5, n_repeats = 1, random_state = 42)\n",
    "# ridge_cv_random = RidgeCV(cv=cv, alphas=[0.001, 0.01, 1, 10])\n",
    "# ridge_cv_random = RidgeCV(cv=cv, alphas=np.logspace(-8, 8, base=10, num=17))\n",
    "# ridge_cv_random = RidgeCV(\n",
    "#     cv=5, \n",
    "#     alphas=np.logspace(-5, 5, base=10, num=11),\n",
    "#     scoring='r2',\n",
    "#     scoring='max_error',\n",
    "#     scoring='explained_variance'\n",
    "# )\n",
    "ridge_cv_random = RidgeCV(cv=5, alphas=np.logspace(-8, 8, base=10, num=17))\n",
    "ridge_cv_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9d48b-28dd-47ec-9098-970958bc9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Estimated regularization parameter {ridge_cv_random.alpha_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c58e251",
   "metadata": {},
   "source": [
    "### Validation set $R^2$ performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536045c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Validation R2 performance {ridge_cv_random.best_score_:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680ac1d1",
   "metadata": {},
   "source": [
    "### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fcd436",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.maximum(ridge_cv_random.predict(x_train), 0)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "# fig, ax = plt.figure()\n",
    "plt.scatter(y_pred, y_train, alpha=1, s=4)\n",
    "plt.xlabel(\"Predicted\", fontsize=15, x = .3)\n",
    "plt.ylabel(\"Ground Truth\", fontsize=15)\n",
    "plt.suptitle(r\"$\\log_{10}(1 + Crop Yield)$\", fontsize=20, y=1.02)\n",
    "plt.title((f\"Model applied to train data n = {len(x_train)}, R$^2$ = {(r2_score(y_train, y_pred)):0.2f}\"),\n",
    "          fontsize=12, y=1.01)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "ax.axline([0, 0], [1, 1])\n",
    "m, b = np.polyfit(y_pred, y_train, 1)\n",
    "plt.plot(y_pred, m * y_pred + b, color=\"black\")\n",
    "plt.gca().spines.right.set_visible(False)\n",
    "plt.gca().spines.top.set_visible(False)\n",
    "\n",
    "\n",
    "# plt.savefig(f'images/{feature_file_name}_train_data.jpg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "# the model is plotted with a black trendline and a blue 45 degree line that serves as a reference of what a perfect correlation would look like\n",
    "# deviation of the lines indicates that there is not a perfect correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbc0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training R^2 = {r2_score(y_train, y_pred):0.2f}\\nPearsons r = {pearsonr(y_pred, y_train)[0]:0.2f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550c544-4a28-4d34-841a-837223fa0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearsonr(y_pred, y_train)[0]  ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c01413-8e64-4ba8-b61e-5fd8c9d10c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_random.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff83102",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb42c16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = np.maximum(ridge_cv_random.predict(x_test), 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(y_pred, y_test, alpha=1, s=4)\n",
    "plt.xlabel(\"Predicted\", fontsize=15)\n",
    "plt.ylabel(\"Ground Truth\", fontsize=15)\n",
    "plt.suptitle(r\"$\\log_{10}(1 + Crop Yield)$\", fontsize=20, y=1.02)\n",
    "plt.title(f\"Model applied to test data n = {len(x_test)}, R$^2$ = {(r2_score(y_test, y_pred)):0.2f}\",\n",
    "          fontsize=12, y=1)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "m, b = np.polyfit(y_pred, y_test, 1)\n",
    "plt.plot(y_pred, m * y_pred + b, color=\"black\")\n",
    "plt.gca().spines.right.set_visible(False)\n",
    "plt.gca().spines.top.set_visible(False)\n",
    "\n",
    "# plt.savefig(f'images/{feature_file_name}_test_data.jpg', dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Testing set R^2 = {r2_score(y_test, y_pred):0.2f}\")\n",
    "print(f\"Testing set pearsons R = {pearsonr(y_pred, y_test)[0]:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29cb5b",
   "metadata": {},
   "source": [
    "### Plot the fitted features\n",
    "\n",
    "# add to this markdown chunk to explain what is happening in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360ddd6-1c76-4aa0-90cf-3b6ff35b1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall the object we created earlier, before we split the features by year into those that would train the model and those that would be fed into the trained model to predict crop yields in years for which we do not have crop data\n",
    "#features_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d9c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy this copy and rename it meaningfully\n",
    "# this makes it easier to change the following code chunks & start over with modifications if you made an error when fitting, etc. without needing to rerun the entire notebook to reset important object\n",
    "features_for_2014_2021_predictions = features_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_all = features_for_predictions.drop([\n",
    "#    'year', \n",
    "#    'geometry',\n",
    "#    'district',\n",
    "#    'crop_perc'\n",
    "#], axis = 1)\n",
    "#features_for_predictions['fit'] = np.maximum(ridge_cv_random.predict(x_all), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bd093",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_for_predictions = geopandas.GeoDataFrame(features_for_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f8b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_for_predictions['fit'].mask(features_for_predictions['crop_perc']==0, 0, inplace=True)\n",
    "# pred_features.loc[pred_features[\"crop_perc\"] == 0, \"fit\"] = 0   ### Does same thing but differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0592486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_features = pred_features[pred_features.crop_perc > 0].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7dc378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_for_predictions['fit'].mask(features_for_predictions['fit'] > 2, 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7780981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_for_predictions = features_for_predictions[features_for_predictions.year == 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_for_predictions.plot(figsize = (10,10),\n",
    "#                   marker='H',\n",
    "#                   legend = True,\n",
    "#                   markersize = marker_sz,\n",
    "#                    alpha = .9,\n",
    "#                   column = 'fit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bb5cd2-493d-4172-8454-bf7d5c813a86",
   "metadata": {},
   "source": [
    "### Run trained model on features for 2019 - 2021\n",
    "\n",
    "The features we plug into the model here, `features_for_predictions` represents the years not summarized to administrative boundary level or joined to the crop data, `2019-2021`. Recall that we only have crop data available for Zambia for `2016-2018`, which is why the features for those years are what we used to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e2252-ecff-4f26-a7c6-1060cf0eb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remind yourself of the shape of this dataframe\n",
    "features_without_crop_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cef61e3-8fff-4be8-8269-d136de077843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remind yourself of the columns in this dataframe\n",
    "features_without_crop_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03595b08-f243-458e-abcc-48471ad5a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we remove several columns in the next step, save the `district` and `year` columns as separate objects that will then be re-joined to the predictions that result from the features\n",
    "districts = features_without_crop_data['district']\n",
    "years = features_without_crop_data['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7c6e84-b3d9-4118-a88f-79d10156c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the districts object we just created\n",
    "districts.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca8528c-c848-4d8e-95bb-c418f44024fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the year object we just created\n",
    "years.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18425b93-46fd-4add-8081-f4c78a4729b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_without_crop_data = features_without_crop_data.drop([\n",
    "    'district', \n",
    "    'year', \n",
    "    \"crop_perc\"\n",
    "], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa6546b-a6c7-4a9e-8490-bdeb2ac1f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the only columns are all features\n",
    "features_without_crop_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f6b09-3e6f-4762-9942-34b09c2c8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for the features that were processed for this purpose\n",
    "# need to execute the ridge_cv_random within the np.maximum() function because some predictions results are negative and we need them all to be positive because conceptually crop yields cannot be negative\n",
    "pred_2019_2021 = np.maximum(ridge_cv_random.predict(features_without_crop_data), 0)\n",
    "pred_2019_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdba2ca-292a-4e9a-ad5f-2b8722efab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate check:\n",
    "# confirm that the length of the array of predicitions is the same number of rows as the feature data fed into the model\n",
    "length_predictions = len(pred_2019_2021)\n",
    "length_features = len(features_without_crop_data)\n",
    "# the next line will return True if they are the same length or False if not\n",
    "length_predictions == length_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f82e7a-adb1-4dbe-adcd-2140597482b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the predictions from an array into a gdf so we can re-join the points and year variables to plot \n",
    "pred_2019_2021_gdf = geopandas.GeoDataFrame(pred_2019_2021)\n",
    "pred_2019_2021_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24f15fb-fcd9-4f6d-8362-b437a8f19e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column for predictions just for clarity, this duplicates the column 0 so we need to drop in the next chunk\n",
    "pred_2019_2021_gdf['pred_yields'] = pred_2019_2021_gdf[0]\n",
    "pred_2019_2021_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c225c-d845-4ae0-8937-efa40d42a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop one duplicated column, the one with the non-descriptive name\n",
    "pred_2019_2021_gdf = pred_2019_2021_gdf.drop([0], axis = 1)\n",
    "pred_2019_2021_gdf\n",
    "# recall that the units are metric tonnes per hectare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8332c-1788-4912-81e3-c66f4fac655a",
   "metadata": {},
   "source": [
    "# ATTENTION: Before executing the next chunk we need to reassign the object `points` to those from the dataframe that is only years 2019-2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972bc07-ec1c-437a-9d49-90b44b0f8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make a copy of the gdf but we will move forward with processing the original file\n",
    "pred_2019_2021_gdf_copy = pred_2019_2021_gdf.copy()\n",
    "\n",
    "# rejoin the points geometry and year columns so we can plot both years independently\n",
    "# recall that we made this `points` objects directly from the `features` dataframe, which was the parent dataframe of the one fed into the model to produce the predictions that we are processing here\n",
    "pred_2019_2021_gdf['geometry'] = points.geometry\n",
    "pred_2019_2021_gdf['year'] = year\n",
    "# check that the geometries were properly rejoined\n",
    "pred_2019_2021_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6916f-da2a-4e7e-91b5-2e984b773610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the data by year, we care most about 2019, 2020, and 2021 so these are the default years to subset the predictions df\n",
    "\n",
    "#yr = 2014\n",
    "#yr = 2015\n",
    "#yr = 2016\n",
    "#yr = 2017\n",
    "yr = 2018\n",
    "#yr = 2019\n",
    "#yr = 2020\n",
    "#yr = 2021\n",
    "\n",
    "pred_yield = pred_14_21_gdf[pred_14_21_gdf.year == yr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a93557-c3df-406d-8251-0b82a6e0fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382329de-6bb9-45e3-a2c6-7f216016ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 2019 crop predictions\n",
    "pred_yield.plot(figsize= (15,15),\n",
    "                  marker = 'H',\n",
    "                  legend = True,\n",
    "                  markersize = 20,\n",
    "                  column = \"pred_yields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc95958-648c-439f-96f9-a7d828e9d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1db332-5b3f-4444-87ef-b99cb195c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b2320",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Yield and Residual Plots\n",
    "### Create data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = features_summary.drop(drop_cols, axis = 1)\n",
    "\n",
    "residual_df = pd.DataFrame()\n",
    "\n",
    "residual_df[\"yield_mt\"] = features_summary.yield_mt.to_numpy()\n",
    "residual_df[\"log_yield\"] = np.log10(features_summary.yield_mt.to_numpy() + 1)\n",
    "residual_df[\"prediction\"] = np.maximum(ridge_cv_random.predict(x_all), 0)\n",
    "residual_df[\"residual\"] = residual_df[\"log_yield\"] - residual_df[\"prediction\"]\n",
    "residual_df[\"year\"] = features_summary.year\n",
    "residual_df[\"district\"] = features_summary.district\n",
    "residual_df = residual_df.join(country_shp, how = \"left\", on = \"district\")\n",
    "#demean by location\n",
    "residual_df[\"district_yield_mean\"] = residual_df.groupby('district')['log_yield'].transform('mean')\n",
    "residual_df[\"district_prediction_mean\"] = residual_df.groupby('district')['prediction'].transform('mean')\n",
    "residual_df[\"demean_yield\"] = residual_df[\"log_yield\"] - residual_df[\"district_yield_mean\"]\n",
    "residual_df[\"demean_prediction\"] = residual_df[\"prediction\"] - residual_df[\"district_prediction_mean\"]\n",
    "residual_gdf = geopandas.GeoDataFrame(residual_df)\n",
    "# residual_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f135fc",
   "metadata": {},
   "source": [
    "### Crop yield histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce9cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    residual_gdf, \n",
    "    col=\"year\", \n",
    "#     col_wrap = 3, \n",
    "    height=4, \n",
    "    aspect=1\n",
    ")\n",
    "g.map(sns.histplot, \"yield_mt\", bins = 20)\n",
    "g.set_axis_labels(\"Yield (MT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e79fc",
   "metadata": {},
   "source": [
    "### Log transform crop yield histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50970cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    residual_gdf, \n",
    "    col=\"year\", \n",
    "#     col_wrap = 3, \n",
    "    height=4, \n",
    "    aspect=1\n",
    ")\n",
    "g.map(sns.histplot, \"log_yield\", bins = 20)\n",
    "g.set_axis_labels(r\"$\\log_{10}(1 + Crop Yield)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e9988",
   "metadata": {},
   "source": [
    "### Crop prediction histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    residual_gdf, \n",
    "    col=\"year\", \n",
    "#     col_wrap = 3, \n",
    "    height=4, \n",
    "    aspect=1\n",
    ")\n",
    "g.map(sns.histplot, \"prediction\", bins = 20)\n",
    "g.set_axis_labels(r\"Crop yield predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdff1e51",
   "metadata": {},
   "source": [
    "### Residual histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85090f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    residual_gdf, \n",
    "    col=\"year\", \n",
    "#     col_wrap = 3, \n",
    "    height=4, \n",
    "    aspect=1\n",
    ")\n",
    "g.map(sns.histplot, \"residual\", bins = 20)\n",
    "g.set_axis_labels(r\"Residuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1998e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_gdf.residual.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23318687",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_gdf.residual.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d523df",
   "metadata": {},
   "source": [
    "### Log rop yield vs residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ecd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    residual_gdf, \n",
    "    col=\"year\", \n",
    "#     col_wrap = 3, \n",
    "    height=4, \n",
    "    aspect=1\n",
    ")\n",
    "g.map(sns.scatterplot, \"log_yield\", \"residual\")\n",
    "g.set_axis_labels(r\"$\\log_{10}(1 + Crop Yield)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96e382",
   "metadata": {},
   "source": [
    "### District residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b750cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if satellite == 'landsat-8-c2-l2':\n",
    "    fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(13, 5))\n",
    "    ax1 = (residual_gdf[residual_gdf.year == 2014]\n",
    "           .plot(ax = ax1, column = \"residual\", legend = True, norm=colors.Normalize(vmin= -0.4, vmax=0.4), cmap = \"BrBG\")\n",
    "           .set_title(\"2014 Residuals\"))\n",
    "    ax2 = (residual_gdf[residual_gdf.year == 2015]\n",
    "           .plot(ax = ax2, column = \"residual\", legend = True, norm=colors.Normalize(vmin= -0.4, vmax=0.4), cmap = \"BrBG\")\n",
    "           .set_title(\"2015 Residuals\"))\n",
    "else:\n",
    "    pass\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "ax1 = (residual_gdf[residual_gdf.year == 2016]\n",
    "       .plot(ax = ax1, column = \"residual\", legend = True, norm=colors.Normalize(vmin= -0.4, vmax=0.4), cmap = \"BrBG\")\n",
    "       .set_title(\"2016 Residuals\"))\n",
    "ax2 = (residual_gdf[residual_gdf.year == 2017]\n",
    "       .plot(ax = ax2, column = \"residual\", legend = True, norm=colors.Normalize(vmin= -0.4, vmax=0.4), cmap = \"BrBG\")\n",
    "       .set_title(\"2017 Residuals\"))\n",
    "ax3 = (residual_gdf[residual_gdf.year == 2018]\n",
    "       .plot(ax = ax3, column = \"residual\", legend = True, norm=colors.Normalize(vmin= -0.4, vmax=0.4), cmap = \"BrBG\")\n",
    "       .set_title(\"2018 Residuals\"))\n",
    "\n",
    "caption = \"A positive value is an underestimated prediction (the prediction is lower than the actual yield), a negative value is an over estimated prediction\"\n",
    "plt.figtext(0.5, 0.01, caption, wrap=True, horizontalalignment='center', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ddf373",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Difference from the Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bca65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    residual_gdf, \n",
    "    col=\"year\", \n",
    "#     col_wrap = 3, \n",
    "    height=4, \n",
    "    aspect=1\n",
    ")\n",
    "g.map(sns.scatterplot, \"demean_yield\", \"demean_prediction\")\n",
    "g.set_axis_labels('Difference from Yield Mean', 'Difference from Prediction Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(residual_gdf.demean_yield, residual_gdf.demean_prediction)\n",
    "plt.title(\"Demeaned truth and predictions by district\")\n",
    "plt.xlabel('Difference from Yield Mean')\n",
    "plt.ylabel('Difference from Predictions Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acc3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for yr in range(year_start+1, year_end+1):\n",
    "    r_squared = r2_score(residual_gdf[residual_gdf.year == yr][\"demean_yield\"], residual_gdf[residual_gdf.year == yr][\"demean_prediction\"])\n",
    "    pearson_r = pearsonr(residual_gdf[residual_gdf.year == yr][\"demean_yield\"], residual_gdf[residual_gdf.year == yr][\"demean_prediction\"])\n",
    "    \n",
    "    print(yr, f\"    R^2: {r_squared:.2f}\\n\",\n",
    "          f\"Pearson's r: {pearson_r[0]:.2f}\\n\", \n",
    "          sep = \"\")\n",
    "    \n",
    "r_squared = r2_score(residual_gdf[\"demean_yield\"], residual_gdf[\"demean_prediction\"])\n",
    "pearson_r = pearsonr(residual_gdf[\"demean_yield\"], residual_gdf[\"demean_prediction\"])\n",
    "print(f\"All     R^2: {r_squared:.2f}\\n\",\n",
    "      f\"Pearson's r: {pearson_r[0]:.2f}\", sep = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pearson_r[0] ** 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9fb972-d73c-468c-9bfa-a658e839df85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
